#!/usr/bin/env python3
"""
SEO å¯©è¨ˆè…³æœ¬
æƒææ‰€æœ‰ .njk é é¢æ–‡ä»¶ï¼Œåˆ†æ SEO å…ƒç´ ï¼Œç”Ÿæˆè©³ç´°å¯©è¨ˆå ±å‘Š
åŒ…å« Schema.org çµæ§‹åŒ–æ•¸æ“šå»ºè­°
"""

import re
import json
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict
from datetime import datetime

# å°ˆæ¡ˆæ ¹ç›®éŒ„
PROJECT_ROOT = Path(__file__).parent.parent
SRC_DIR = PROJECT_ROOT / "src"
REPORT_DIR = PROJECT_ROOT / "report"

# ç¶²ç«™ URLï¼ˆå¾ metadata.json è®€å–ï¼‰
SITE_URL = "https://goldenyearsphoto.com"

# SEO æœ€ä½³å¯¦è¸æ¨™æº–
SEO_STANDARDS = {
    'title': {
        'min_length': 30,
        'max_length': 60,
        'optimal_length': 50,
    },
    'description': {
        'min_length': 120,
        'max_length': 160,
        'optimal_length': 150,
    },
    'keywords': {
        'min_count': 5,
        'max_count': 10,
    },
}


class SEOAuditor:
    """SEO å¯©è¨ˆå™¨"""
    
    def __init__(self, src_dir: Path, site_url: str):
        self.src_dir = src_dir
        self.site_url = site_url
        self.pages: List[Dict[str, Any]] = []
        self.issues: List[Dict[str, Any]] = []
        self.schema_recommendations: Dict[str, Any] = {}
        
    def audit(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´ SEO å¯©è¨ˆ"""
        print("ğŸ” é–‹å§‹ SEO å¯©è¨ˆ...\n")
        
        # 1. æƒææ‰€æœ‰ .njk é é¢æ–‡ä»¶
        print("ğŸ“ æƒæé é¢æ–‡ä»¶...")
        page_files = self._scan_pages()
        print(f"   æ‰¾åˆ° {len(page_files)} å€‹é é¢æ–‡ä»¶\n")
        
        # 2. è§£ææ¯å€‹é é¢çš„ front matter
        print("ğŸ“„ è§£æé é¢ front matter...")
        for file_path in page_files:
            page_data = self._parse_page(file_path)
            if page_data:
                self.pages.append(page_data)
        print(f"   æˆåŠŸè§£æ {len(self.pages)} å€‹é é¢\n")
        
        # 3. åˆ†æ SEO å…ƒç´ 
        print("ğŸ” åˆ†æ SEO å…ƒç´ ...")
        for page in self.pages:
            page['seo_analysis'] = self._analyze_seo(page)
            page['schema_recommendation'] = self._recommend_schema(page)
        print("   SEO åˆ†æå®Œæˆ\n")
        
        # 4. æª¢æŸ¥é‡è¤‡å’Œä¸€è‡´æ€§
        print("ğŸ”— æª¢æŸ¥é‡è¤‡å’Œä¸€è‡´æ€§...")
        self._check_duplicates()
        print("   æª¢æŸ¥å®Œæˆ\n")
        
        # 5. ç”Ÿæˆç¸½é«”çµ±è¨ˆ
        print("ğŸ“Š ç”Ÿæˆçµ±è¨ˆæ•¸æ“š...")
        stats = self._generate_stats()
        print("   çµ±è¨ˆå®Œæˆ\n")
        
        return {
            'timestamp': datetime.now().isoformat(),
            'site_url': self.site_url,
            'total_pages': len(self.pages),
            'pages': self.pages,
            'stats': stats,
            'issues': self.issues,
            'schema_recommendations': self.schema_recommendations,
        }
    
    def _scan_pages(self) -> List[Path]:
        """æƒææ‰€æœ‰ .njk é é¢æ–‡ä»¶"""
        pages = []
        
        for file_path in self.src_dir.rglob("*.njk"):
            # æ’é™¤æ¨¡æ¿å’Œéƒ¨åˆ†æ–‡ä»¶
            rel_path = file_path.relative_to(self.src_dir)
            if str(rel_path).startswith("_includes/"):
                continue
            if str(rel_path).startswith("_data/"):
                continue
            pages.append(file_path)
        
        return sorted(pages)
    
    def _parse_page(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """è§£æé é¢çš„ front matter"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå– front matterï¼ˆYAML åœ¨ --- ä¹‹é–“ï¼‰
            front_matter_match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)$', content, re.DOTALL)
            
            if not front_matter_match:
                return None
            
            front_matter_text = front_matter_match.group(1)
            body_content = front_matter_match.group(2)
            
            # è§£æ YAMLï¼ˆä½¿ç”¨ç°¡å–®è§£æå™¨ï¼‰
            try:
                front_matter = self._parse_simple_yaml(front_matter_text)
            except Exception as e:
                print(f"   âš ï¸  YAML è§£æéŒ¯èª¤ ({file_path.relative_to(PROJECT_ROOT)}): {e}")
                return None
            
            # æ¨æ–· URL
            rel_path = file_path.relative_to(self.src_dir)
            url = self._infer_url(rel_path, front_matter)
            
            # æ¨æ–·é é¢é¡å‹
            page_type = self._infer_page_type(rel_path, front_matter)
            
            # æå– H1 æ¨™ç±¤ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            h1_tags = self._extract_h1_tags(body_content)
            
            return {
                'file_path': str(file_path.relative_to(PROJECT_ROOT)),
                'rel_path': str(rel_path),
                'url': url,
                'page_type': page_type,
                'front_matter': front_matter,
                'title': front_matter.get('title', ''),
                'seo': front_matter.get('seo', {}),
                'h1_tags': h1_tags,
                'body_length': len(body_content),
            }
        except Exception as e:
            print(f"   âš ï¸  è§£æéŒ¯èª¤ ({file_path.relative_to(PROJECT_ROOT)}): {e}")
            return None
    
    def _infer_url(self, rel_path: Path, front_matter: Dict) -> str:
        """æ¨æ–·é é¢ URL"""
        # å„ªå…ˆä½¿ç”¨ front matter ä¸­çš„ permalink
        if 'permalink' in front_matter:
            return front_matter['permalink']
        
        # æ ¹æ“šæ–‡ä»¶è·¯å¾‘æ¨æ–·
        url_path = str(rel_path).replace('\\', '/')
        
        # ç§»é™¤ .njk æ“´å±•å
        if url_path.endswith('.njk'):
            url_path = url_path[:-4]
        
        # index.njk è½‰æ›ç‚ºç›®éŒ„
        if url_path.endswith('/index'):
            url_path = url_path[:-6]
        elif url_path == 'index':
            url_path = ''
        
        # æ§‹å»ºå®Œæ•´ URL
        url = f"{self.site_url}/{url_path}" if url_path else self.site_url
        return url.rstrip('/') + '/'
    
    def _infer_page_type(self, rel_path: Path, front_matter: Dict) -> str:
        """æ¨æ–·é é¢é¡å‹"""
        rel_str = str(rel_path).lower()
        
        if rel_str == 'index.njk':
            return 'home'
        elif 'services/' in rel_str:
            return 'service'
        elif 'blog/' in rel_str:
            return 'blog'
        elif 'guide/' in rel_str:
            return 'guide'
        elif 'booking/' in rel_str:
            return 'booking'
        elif 'about' in rel_str:
            return 'about'
        elif 'price-list' in rel_str or 'price' in rel_str:
            return 'pricing'
        else:
            return 'other'
    
    def _parse_simple_yaml(self, yaml_text: str) -> Dict[str, Any]:
        """ç°¡å–®çš„ YAML è§£æå™¨ï¼ˆç”¨æ–¼è§£æ front matterï¼‰
        æ”¯æŒåŸºæœ¬æ ¼å¼ï¼škey: value å’ŒåµŒå¥—çµæ§‹ï¼ˆseo: description: ...ï¼‰
        """
        result = {}
        lines = yaml_text.split('\n')
        i = 0
        
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()
            
            # è·³éç©ºè¡Œå’Œè¨»é‡‹
            if not stripped or stripped.startswith('#'):
                i += 1
                continue
            
            # è¨ˆç®—ç¸®é€²ï¼ˆä½¿ç”¨ç©ºæ ¼æ•¸ï¼‰
            indent = len(line) - len(line.lstrip())
            
            # è§£æ key: value
            if ':' in line:
                colon_idx = line.index(':')
                key = line[:colon_idx].strip()
                value_part = line[colon_idx + 1:].strip()
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯åµŒå¥—å°è±¡ï¼ˆä¸‹ä¸€è¡Œæœ‰æ›´å¤šç¸®é€²ï¼‰
                is_object = False
                if i + 1 < len(lines):
                    next_line = lines[i + 1]
                    next_stripped = next_line.strip()
                    if next_stripped and ':' in next_line:
                        next_indent = len(next_line) - len(next_line.lstrip())
                        if next_indent > indent:
                            is_object = True
                
                if is_object:
                    # é€™æ˜¯ä¸€å€‹å°è±¡ï¼Œéæ­¸è§£æ
                    nested_lines = []
                    i += 1
                    while i < len(lines):
                        nested_line = lines[i]
                        nested_stripped = nested_line.strip()
                        if not nested_stripped or nested_stripped.startswith('#'):
                            i += 1
                            continue
                        nested_indent = len(nested_line) - len(nested_line.lstrip())
                        if nested_indent <= indent:
                            break
                        nested_lines.append(nested_line)
                        i += 1
                    i -= 1  # å›é€€ä¸€è¡Œï¼Œå› ç‚ºå¤–å±¤å¾ªç’°æœƒå¢åŠ 
                    
                    nested_text = '\n'.join(nested_lines)
                    result[key] = self._parse_simple_yaml(nested_text)
                else:
                    # è™•ç†æ™®é€šå€¼
                    value = value_part
                    
                    # è™•ç†å¤šè¡Œå€¼ï¼ˆ| æˆ– >ï¼‰
                    if value in ('|', '>'):
                        multiline_lines = []
                        i += 1
                        while i < len(lines):
                            next_line = lines[i]
                            next_indent = len(next_line) - len(next_line.lstrip())
                            if next_indent <= indent and next_line.strip():
                                break
                            if next_indent > indent:
                                multiline_lines.append(next_line[indent:])
                            i += 1
                        value = '\n'.join(multiline_lines).strip()
                        if value.startswith('|') or value.startswith('>'):
                            value = value[1:].lstrip()
                        i -= 1
                    else:
                        # ç§»é™¤å¼•è™Ÿ
                        if (value.startswith('"') and value.endswith('"')) or \
                           (value.startswith("'") and value.endswith("'")):
                            value = value[1:-1]
                    
                    result[key] = value
            
            i += 1
        
        return result
    
    def _extract_h1_tags(self, content: str) -> List[str]:
        """æå– H1 æ¨™ç±¤"""
        h1_pattern = r'<h1[^>]*>(.*?)</h1>'
        matches = re.findall(h1_pattern, content, re.DOTALL | re.IGNORECASE)
        # æ¸…ç† HTML æ¨™ç±¤
        h1_tags = [re.sub(r'<[^>]+>', '', h1).strip() for h1 in matches]
        return h1_tags
    
    def _analyze_seo(self, page: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé é¢çš„ SEO å…ƒç´ """
        analysis = {
            'title_score': 0,
            'description_score': 0,
            'keywords_score': 0,
            'h1_score': 0,
            'overall_score': 0,
            'issues': [],
            'recommendations': [],
        }
        
        # 1. Title åˆ†æ
        title = page.get('title', '')
        title_analysis = self._analyze_title(title)
        analysis['title'] = title_analysis
        analysis['title_score'] = title_analysis['score']
        
        # 2. Description åˆ†æ
        description = page.get('seo', {}).get('description', '')
        desc_analysis = self._analyze_description(description)
        analysis['description'] = desc_analysis
        analysis['description_score'] = desc_analysis['score']
        
        # 3. Keywords åˆ†æ
        keywords = page.get('seo', {}).get('keywords', '')
        keywords_analysis = self._analyze_keywords(keywords)
        analysis['keywords'] = keywords_analysis
        analysis['keywords_score'] = keywords_analysis['score']
        
        # 4. H1 åˆ†æ
        h1_analysis = self._analyze_h1(page.get('h1_tags', []))
        analysis['h1'] = h1_analysis
        analysis['h1_score'] = h1_analysis['score']
        
        # 5. è¨ˆç®—ç¸½é«”åˆ†æ•¸
        analysis['overall_score'] = (
            analysis['title_score'] * 0.3 +
            analysis['description_score'] * 0.35 +
            analysis['keywords_score'] * 0.15 +
            analysis['h1_score'] * 0.2
        )
        
        # æ”¶é›†å•é¡Œå’Œå»ºè­°
        if title_analysis['issues']:
            analysis['issues'].extend(title_analysis['issues'])
        if desc_analysis['issues']:
            analysis['issues'].extend(desc_analysis['issues'])
        if keywords_analysis['issues']:
            analysis['issues'].extend(keywords_analysis['issues'])
        if h1_analysis['issues']:
            analysis['issues'].extend(h1_analysis['issues'])
        
        if title_analysis['recommendations']:
            analysis['recommendations'].extend(title_analysis['recommendations'])
        if desc_analysis['recommendations']:
            analysis['recommendations'].extend(desc_analysis['recommendations'])
        if keywords_analysis['recommendations']:
            analysis['recommendations'].extend(keywords_analysis['recommendations'])
        if h1_analysis['recommendations']:
            analysis['recommendations'].extend(h1_analysis['recommendations'])
        
        return analysis
    
    def _analyze_title(self, title: str) -> Dict[str, Any]:
        """åˆ†æ Title æ¨™ç±¤"""
        analysis = {
            'value': title,
            'length': len(title),
            'score': 0,
            'issues': [],
            'recommendations': [],
        }
        
        if not title:
            analysis['issues'].append({
                'priority': 'high',
                'message': 'ç¼ºå°‘ Title æ¨™ç±¤',
            })
            return analysis
        
        # é•·åº¦æª¢æŸ¥
        length = len(title)
        std = SEO_STANDARDS['title']
        
        if length < std['min_length']:
            analysis['issues'].append({
                'priority': 'high',
                'message': f'Title éçŸ­ï¼ˆ{length} å­—ç¬¦ï¼Œå»ºè­°è‡³å°‘ {std["min_length"]} å­—ç¬¦ï¼‰',
            })
            score = (length / std['min_length']) * 50
        elif length > std['max_length']:
            analysis['issues'].append({
                'priority': 'medium',
                'message': f'Title éé•·ï¼ˆ{length} å­—ç¬¦ï¼Œå»ºè­°ä¸è¶…é {std["max_length"]} å­—ç¬¦ï¼‰',
            })
            # è¶…éé•·åº¦æ™‚åˆ†æ•¸éæ¸›
            excess = length - std['max_length']
            score = max(50, 100 - (excess * 2))
        else:
            # æœ€ä½³é•·åº¦ç¯„åœå…§
            if std['min_length'] <= length <= std['optimal_length']:
                score = 100
            else:
                # åœ¨ optimal å’Œ max ä¹‹é–“ï¼Œåˆ†æ•¸ç•¥å¾®é™ä½
                score = 90
            analysis['recommendations'].append({
                'priority': 'low',
                'message': f'Title é•·åº¦è‰¯å¥½ï¼ˆ{length} å­—ç¬¦ï¼‰',
            })
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å“ç‰Œå
        if 'å¥½æ™‚æœ‰å½±' not in title and 'Golden Years' not in title:
            analysis['recommendations'].append({
                'priority': 'low',
                'message': 'å»ºè­°åœ¨ Title ä¸­åŒ…å«å“ç‰Œåç¨±',
            })
            score *= 0.9
        
        # æª¢æŸ¥ç‰¹æ®Šå­—ç¬¦
        if 'ï½œ' in title or '|' in title:
            # åˆ†éš”ç¬¦ä½¿ç”¨è‰¯å¥½
            pass
        else:
            analysis['recommendations'].append({
                'priority': 'low',
                'message': 'å¯ä»¥è€ƒæ…®ä½¿ç”¨åˆ†éš”ç¬¦ï¼ˆï½œæˆ– |ï¼‰åˆ†éš”å“ç‰Œå’Œé é¢æ¨™é¡Œ',
            })
        
        analysis['score'] = round(score, 1)
        return analysis
    
    def _analyze_description(self, description: str) -> Dict[str, Any]:
        """åˆ†æ Meta Description"""
        analysis = {
            'value': description,
            'length': len(description),
            'score': 0,
            'issues': [],
            'recommendations': [],
        }
        
        if not description:
            analysis['issues'].append({
                'priority': 'high',
                'message': 'ç¼ºå°‘ Meta Description',
            })
            return analysis
        
        # é•·åº¦æª¢æŸ¥
        length = len(description)
        std = SEO_STANDARDS['description']
        
        if length < std['min_length']:
            analysis['issues'].append({
                'priority': 'high',
                'message': f'Description éçŸ­ï¼ˆ{length} å­—ç¬¦ï¼Œå»ºè­°è‡³å°‘ {std["min_length"]} å­—ç¬¦ï¼‰',
            })
            score = (length / std['min_length']) * 60
        elif length > std['max_length']:
            analysis['issues'].append({
                'priority': 'medium',
                'message': f'Description éé•·ï¼ˆ{length} å­—ç¬¦ï¼Œå»ºè­°ä¸è¶…é {std["max_length"]} å­—ç¬¦ï¼Œå¯èƒ½è¢«æˆªæ–·ï¼‰',
            })
            excess = length - std['max_length']
            score = max(60, 100 - (excess * 2))
        else:
            score = 100
            analysis['recommendations'].append({
                'priority': 'low',
                'message': f'Description é•·åº¦è‰¯å¥½ï¼ˆ{length} å­—ç¬¦ï¼‰',
            })
        
        # æª¢æŸ¥å…§å®¹è³ªé‡
        # æ˜¯å¦åŒ…å« CTA è©å½™
        cta_words = ['ç«‹å³', 'æŸ¥çœ‹', 'é ç´„', 'äº†è§£æ›´å¤š', 'é–‹å§‹', 'Get', 'Try', 'Learn']
        has_cta = any(word in description for word in cta_words)
        if not has_cta:
            analysis['recommendations'].append({
                'priority': 'low',
                'message': 'å»ºè­°åœ¨ Description ä¸­åŒ…å«è¡Œå‹•å‘¼ç±²ï¼ˆCTAï¼‰',
            })
            score *= 0.95
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµè©
        if length > 50:
            # å°æ–¼è¼ƒé•·çš„æè¿°ï¼Œé—œéµè©æª¢æŸ¥ä¸é‚£éº¼é‡è¦
            pass
        
        analysis['score'] = round(score, 1)
        return analysis
    
    def _analyze_keywords(self, keywords: str) -> Dict[str, Any]:
        """åˆ†æ Keywords"""
        analysis = {
            'value': keywords,
            'count': 0,
            'score': 0,
            'issues': [],
            'recommendations': [],
        }
        
        if not keywords:
            # Keywords ä¸æ˜¯å¿…é ˆçš„ï¼Œä½†å»ºè­°æä¾›
            analysis['recommendations'].append({
                'priority': 'low',
                'message': 'å»ºè­°æ·»åŠ  keywords å­—æ®µï¼ˆé›–ç„¶ Google ä¸å†ä½¿ç”¨ï¼Œä½†å…¶ä»–æœç´¢å¼•æ“å¯èƒ½ä½¿ç”¨ï¼‰',
            })
            analysis['score'] = 70  # ä¸å½±éŸ¿å¤ªå¤§
            return analysis
        
        # è§£æé—œéµè©ï¼ˆå¯èƒ½æ˜¯é€—è™Ÿåˆ†éš”çš„å­—ä¸²ï¼‰
        if isinstance(keywords, str):
            keyword_list = [k.strip() for k in keywords.split(',') if k.strip()]
        elif isinstance(keywords, list):
            keyword_list = keywords
        else:
            keyword_list = []
        
        analysis['count'] = len(keyword_list)
        std = SEO_STANDARDS['keywords']
        
        if len(keyword_list) < std['min_count']:
            analysis['recommendations'].append({
                'priority': 'low',
                'message': f'é—œéµè©æ•¸é‡è¼ƒå°‘ï¼ˆ{len(keyword_list)} å€‹ï¼Œå»ºè­° {std["min_count"]}-{std["max_count"]} å€‹ï¼‰',
            })
            score = 80
        elif len(keyword_list) > std['max_count']:
            analysis['recommendations'].append({
                'priority': 'low',
                'message': f'é—œéµè©æ•¸é‡è¼ƒå¤šï¼ˆ{len(keyword_list)} å€‹ï¼Œå»ºè­°ä¸è¶…é {std["max_count"]} å€‹ï¼‰',
            })
            score = 85
        else:
            score = 100
        
        analysis['score'] = round(score, 1)
        return analysis
    
    def _analyze_h1(self, h1_tags: List[str]) -> Dict[str, Any]:
        """åˆ†æ H1 æ¨™ç±¤"""
        analysis = {
            'count': len(h1_tags),
            'tags': h1_tags,
            'score': 0,
            'issues': [],
            'recommendations': [],
        }
        
        if len(h1_tags) == 0:
            analysis['issues'].append({
                'priority': 'medium',
                'message': 'é é¢ç¼ºå°‘ H1 æ¨™ç±¤',
            })
            analysis['score'] = 0
        elif len(h1_tags) == 1:
            analysis['score'] = 100
            analysis['recommendations'].append({
                'priority': 'low',
                'message': f'H1 æ¨™ç±¤è‰¯å¥½: "{h1_tags[0]}"',
            })
        else:
            # å¤šå€‹ H1ï¼ˆä¸æ¨è–¦ï¼‰
            analysis['issues'].append({
                'priority': 'medium',
                'message': f'é é¢åŒ…å«å¤šå€‹ H1 æ¨™ç±¤ï¼ˆ{len(h1_tags)} å€‹ï¼‰ï¼Œå»ºè­°åªä½¿ç”¨ä¸€å€‹',
            })
            analysis['score'] = 50
        
        return analysis
    
    def _recommend_schema(self, page: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨è–¦ Schema.org çµæ§‹åŒ–æ•¸æ“š"""
        page_type = page.get('page_type', 'other')
        url = page.get('url', '')
        title = page.get('title', '')
        description = page.get('seo', {}).get('description', '')
        
        schemas = []
        
        if page_type == 'home':
            # Organization + WebSite
            schemas.append({
                '@context': 'https://schema.org',
                '@type': 'Organization',
                'name': 'å¥½æ™‚æœ‰å½± Golden Years',
                'url': self.site_url,
                'logo': f'{self.site_url}/assets/images/ui/å¥½æ™‚æœ‰å½±logoè—å­—é€æ˜åº•.png',
                'sameAs': [
                    'https://www.instagram.com/goldenyears_studio/'
                ],
                'contactPoint': {
                    '@type': 'ContactPoint',
                    'telephone': '+886-2-2709-2224',
                    'contactType': 'customer service',
                    'areaServed': 'TW',
                    'availableLanguage': ['Chinese', 'English']
                },
                'address': {
                    '@type': 'PostalAddress',
                    'addressCountry': 'TW',
                    'addressLocality': 'å°åŒ—å¸‚',
                    'addressRegion': 'å°åŒ—å¸‚',
                }
            })
            
            schemas.append({
                '@context': 'https://schema.org',
                '@type': 'WebSite',
                'name': 'å¥½æ™‚æœ‰å½± Golden Years',
                'url': self.site_url,
                'potentialAction': {
                    '@type': 'SearchAction',
                    'target': {
                        '@type': 'EntryPoint',
                        'urlTemplate': f'{self.site_url}/search?q={{search_term_string}}'
                    },
                    'query-input': 'required name=search_term_string'
                }
            })
            
        elif page_type == 'service':
            # Service + LocalBusiness
            schemas.append({
                '@context': 'https://schema.org',
                '@type': 'Service',
                'name': title,
                'description': description,
                'provider': {
                    '@type': 'LocalBusiness',
                    'name': 'å¥½æ™‚æœ‰å½± Golden Years',
                    'url': self.site_url,
                },
                'areaServed': {
                    '@type': 'City',
                    'name': 'å°åŒ—å¸‚'
                },
                'serviceType': 'Professional Photography'
            })
            
        elif page_type == 'blog':
            # BlogPosting æˆ– Article
            schemas.append({
                '@context': 'https://schema.org',
                '@type': 'BlogPosting',
                'headline': title,
                'description': description,
                'url': url,
                'author': {
                    '@type': 'Organization',
                    'name': 'å¥½æ™‚æœ‰å½± Golden Years'
                },
                'publisher': {
                    '@type': 'Organization',
                    'name': 'å¥½æ™‚æœ‰å½± Golden Years',
                    'logo': {
                        '@type': 'ImageObject',
                        'url': f'{self.site_url}/assets/images/ui/å¥½æ™‚æœ‰å½±logoè—å­—é€æ˜åº•.png'
                    }
                },
                'datePublished': datetime.now().isoformat(),
                'inLanguage': 'zh-Hant'
            })
            
        elif page_type == 'about':
            # AboutPage + Organization
            schemas.append({
                '@context': 'https://schema.org',
                '@type': 'AboutPage',
                'name': title,
                'description': description,
                'url': url,
                'mainEntity': {
                    '@type': 'Organization',
                    'name': 'å¥½æ™‚æœ‰å½± Golden Years',
                    'url': self.site_url,
                }
            })
            
        elif page_type == 'booking':
            # ReservationPage
            schemas.append({
                '@context': 'https://schema.org',
                '@type': 'ReservationPage',
                'name': title,
                'description': description,
                'url': url,
            })
            
        elif page_type == 'pricing':
            # WebPage + PriceSpecification
            schemas.append({
                '@context': 'https://schema.org',
                '@type': 'WebPage',
                'name': title,
                'description': description,
                'url': url,
            })
        
        # å¦‚æœæ²’æœ‰ç‰¹å®šé¡å‹ï¼Œè‡³å°‘æä¾›åŸºæœ¬ WebPage
        if not schemas:
            schemas.append({
                '@context': 'https://schema.org',
                '@type': 'WebPage',
                'name': title,
                'description': description,
                'url': url,
            })
        
        return {
            'recommended_types': [s['@type'] for s in schemas],
            'schema_json': schemas,
            'implementation': self._generate_schema_html(schemas),
        }
    
    def _generate_schema_html(self, schemas: List[Dict]) -> str:
        """ç”Ÿæˆ Schema.org JSON-LD HTML ä»£ç¢¼"""
        html_parts = []
        for schema in schemas:
            json_str = json.dumps(schema, ensure_ascii=False, indent=2)
            html_parts.append(f'<script type="application/ld+json">\n{json_str}\n</script>')
        return '\n\n'.join(html_parts)
    
    def _check_duplicates(self):
        """æª¢æŸ¥é‡è¤‡çš„ title å’Œ description"""
        titles = defaultdict(list)
        descriptions = defaultdict(list)
        
        for page in self.pages:
            title = page.get('title', '')
            if title:
                titles[title].append(page['url'])
            
            description = page.get('seo', {}).get('description', '')
            if description:
                descriptions[description].append(page['url'])
        
        # æª¢æŸ¥é‡è¤‡çš„ title
        for title, urls in titles.items():
            if len(urls) > 1:
                self.issues.append({
                    'type': 'duplicate_title',
                    'priority': 'high',
                    'message': f'æ¨™é¡Œé‡è¤‡: "{title}"',
                    'affected_pages': urls,
                })
        
        # æª¢æŸ¥é‡è¤‡çš„ description
        for desc, urls in descriptions.items():
            if len(urls) > 1:
                self.issues.append({
                    'type': 'duplicate_description',
                    'priority': 'medium',
                    'message': f'æè¿°é‡è¤‡ï¼ˆå‡ºç¾åœ¨ {len(urls)} å€‹é é¢ï¼‰',
                    'affected_pages': urls[:3],  # åªé¡¯ç¤ºå‰3å€‹
                })
    
    def _generate_stats(self) -> Dict[str, Any]:
        """ç”Ÿæˆçµ±è¨ˆæ•¸æ“š"""
        if not self.pages:
            return {}
        
        scores = [p['seo_analysis']['overall_score'] for p in self.pages]
        
        return {
            'average_score': round(sum(scores) / len(scores), 1),
            'min_score': round(min(scores), 1),
            'max_score': round(max(scores), 1),
            'pages_with_issues': sum(1 for p in self.pages if p['seo_analysis']['issues']),
            'total_issues': sum(len(p['seo_analysis']['issues']) for p in self.pages),
            'pages_with_schema_recommendations': len([p for p in self.pages if p.get('schema_recommendation')]),
        }


def generate_markdown_report(audit_result: Dict[str, Any]) -> str:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„å ±å‘Š"""
    lines = []
    
    lines.append("# SEO å¯©è¨ˆå ±å‘Š")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ™‚é–“**: {audit_result['timestamp']}")
    lines.append(f"**ç¶²ç«™ URL**: {audit_result['site_url']}")
    lines.append(f"**ç¸½é é¢æ•¸**: {audit_result['total_pages']}")
    lines.append("")
    
    # ç¸½é«”çµ±è¨ˆ
    stats = audit_result['stats']
    lines.append("## ğŸ“Š ç¸½é«”çµ±è¨ˆ")
    lines.append("")
    lines.append(f"- **å¹³å‡ SEO åˆ†æ•¸**: {stats.get('average_score', 0)}/100")
    lines.append(f"- **æœ€ä½åˆ†æ•¸**: {stats.get('min_score', 0)}/100")
    lines.append(f"- **æœ€é«˜åˆ†æ•¸**: {stats.get('max_score', 0)}/100")
    lines.append(f"- **æœ‰å•é¡Œçš„é é¢**: {stats.get('pages_with_issues', 0)}/{audit_result['total_pages']}")
    lines.append(f"- **ç¸½å•é¡Œæ•¸**: {stats.get('total_issues', 0)}")
    lines.append("")
    
    # é é¢è©³æƒ…
    lines.append("## ğŸ“„ é é¢è©³æƒ…")
    lines.append("")
    
    # æŒ‰åˆ†æ•¸æ’åº
    sorted_pages = sorted(audit_result['pages'], key=lambda x: x['seo_analysis']['overall_score'])
    
    for page in sorted_pages:
        url = page['url']
        title = page.get('title', 'ç„¡æ¨™é¡Œ')
        score = page['seo_analysis']['overall_score']
        analysis = page['seo_analysis']
        
        # åˆ†æ•¸é¡è‰²æ¨™è¨˜
        if score >= 80:
            score_badge = f"ğŸŸ¢ {score}"
        elif score >= 60:
            score_badge = f"ğŸŸ¡ {score}"
        else:
            score_badge = f"ğŸ”´ {score}"
        
        lines.append(f"### {title} {score_badge}")
        lines.append("")
        lines.append(f"- **URL**: {url}")
        lines.append(f"- **æ–‡ä»¶**: `{page['file_path']}`")
        lines.append(f"- **é é¢é¡å‹**: {page.get('page_type', 'unknown')}")
        lines.append("")
        
        # å„é …åˆ†æ•¸
        lines.append("#### åˆ†é …è©•åˆ†")
        lines.append("")
        lines.append(f"- Title: {analysis['title_score']}/100 ({analysis['title']['length']} å­—ç¬¦)")
        lines.append(f"- Description: {analysis['description_score']}/100 ({analysis['description']['length']} å­—ç¬¦)")
        lines.append(f"- Keywords: {analysis['keywords_score']}/100")
        lines.append(f"- H1: {analysis['h1_score']}/100 ({analysis['h1']['count']} å€‹)")
        lines.append("")
        
        # å•é¡Œ
        if analysis['issues']:
            lines.append("#### âš ï¸ å•é¡Œ")
            lines.append("")
            for issue in analysis['issues']:
                priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(issue['priority'], 'âšª')
                lines.append(f"- {priority_emoji} **{issue['priority'].upper()}**: {issue['message']}")
            lines.append("")
        
        # å»ºè­°
        if analysis['recommendations']:
            lines.append("#### ğŸ’¡ å»ºè­°")
            lines.append("")
            for rec in analysis['recommendations']:
                priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(rec['priority'], 'âšª')
                lines.append(f"- {priority_emoji} **{rec['priority'].upper()}**: {rec['message']}")
            lines.append("")
        
        # Schema.org å»ºè­°
        schema_rec = page.get('schema_recommendation', {})
        if schema_rec:
            lines.append("#### ğŸ“‹ Schema.org çµæ§‹åŒ–æ•¸æ“šå»ºè­°")
            lines.append("")
            lines.append(f"**æ¨è–¦é¡å‹**: {', '.join(schema_rec['recommended_types'])}")
            lines.append("")
            lines.append("**å¯¦ç¾ä»£ç¢¼**:")
            lines.append("")
            lines.append("```html")
            lines.append(schema_rec['implementation'])
            lines.append("```")
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    # å…¨å±€å•é¡Œ
    if audit_result['issues']:
        lines.append("## ğŸ”— å…¨å±€å•é¡Œ")
        lines.append("")
        for issue in audit_result['issues']:
            priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(issue['priority'], 'âšª')
            lines.append(f"### {priority_emoji} {issue['message']}")
            lines.append("")
            lines.append("**å—å½±éŸ¿çš„é é¢**:")
            for url in issue.get('affected_pages', [])[:5]:
                lines.append(f"- {url}")
            if len(issue.get('affected_pages', [])) > 5:
                lines.append(f"- ... é‚„æœ‰ {len(issue['affected_pages']) - 5} å€‹é é¢")
            lines.append("")
    
    return '\n'.join(lines)


def main():
    """ä¸»å‡½æ•¸"""
    # è®€å– metadata.json ç²å–ç¶²ç«™ URL
    metadata_file = PROJECT_ROOT / "src" / "_data" / "metadata.json"
    site_url = SITE_URL
    if metadata_file.exists():
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                site_url = metadata.get('url', SITE_URL)
        except:
            pass
    
    # ç¢ºä¿å ±å‘Šç›®éŒ„å­˜åœ¨
    REPORT_DIR.mkdir(exist_ok=True)
    
    # åŸ·è¡Œå¯©è¨ˆ
    auditor = SEOAuditor(SRC_DIR, site_url)
    audit_result = auditor.audit()
    
    # ä¿å­˜ JSON å ±å‘Š
    json_report_path = REPORT_DIR / "seo-audit-report.json"
    with open(json_report_path, 'w', encoding='utf-8') as f:
        json.dump(audit_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON å ±å‘Šå·²ä¿å­˜: {json_report_path}")
    
    # ç”Ÿæˆä¸¦ä¿å­˜ Markdown å ±å‘Š
    md_report = generate_markdown_report(audit_result)
    md_report_path = REPORT_DIR / "seo-audit-report.md"
    with open(md_report_path, 'w', encoding='utf-8') as f:
        f.write(md_report)
    print(f"âœ… Markdown å ±å‘Šå·²ä¿å­˜: {md_report_path}")
    
    # è¼¸å‡ºæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š SEO å¯©è¨ˆæ‘˜è¦")
    print("="*60)
    stats = audit_result['stats']
    print(f"ç¸½é é¢æ•¸: {audit_result['total_pages']}")
    print(f"å¹³å‡åˆ†æ•¸: {stats.get('average_score', 0)}/100")
    print(f"æœ‰å•é¡Œçš„é é¢: {stats.get('pages_with_issues', 0)}/{audit_result['total_pages']}")
    print(f"ç¸½å•é¡Œæ•¸: {stats.get('total_issues', 0)}")
    print("="*60)


if __name__ == '__main__':
    main()
