#!/usr/bin/env python3
"""
å…¨é¢ SEO å¯©è¨ˆèˆ‡è©•åˆ†ç³»çµ±
ç‚ºæ¯å€‹é é¢é€²è¡Œå¤šç¶­åº¦ SEO åˆ†æä¸¦çµ¦å‡ºç¶œåˆè©•åˆ†
"""

import re
import json
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict, Counter
from datetime import datetime
import urllib.parse

# å°ˆæ¡ˆæ ¹ç›®éŒ„
PROJECT_ROOT = Path(__file__).parent.parent
SRC_DIR = PROJECT_ROOT / "src"
REPORT_DIR = PROJECT_ROOT / "report"

# ç¶²ç«™ URL
SITE_URL = "https://goldenyearsphoto.com"

# SEO æœ€ä½³å¯¦è¸æ¨™æº–
SEO_STANDARDS = {
    'title': {
        'min_length': 30,
        'max_length': 60,
        'optimal_length': 50,
    },
    'description': {
        'min_length': 120,
        'max_length': 160,
        'optimal_length': 150,
    },
    'keywords': {
        'min_count': 5,
        'max_count': 10,
    },
    'content': {
        'min_length': 300,  # æœ€å°‘å…§å®¹é•·åº¦
        'optimal_length': 1000,
    },
    'heading_structure': {
        'max_h1': 1,
        'prefer_h2_after_h1': True,
    },
    'images': {
        'require_alt': True,
    },
    'links': {
        'min_internal_links': 2,
        'max_external_links': 10,
    },
}


class ComprehensiveSEOAuditor:
    """å…¨é¢ SEO å¯©è¨ˆå™¨"""
    
    def __init__(self, src_dir: Path, site_url: str):
        self.src_dir = src_dir
        self.site_url = site_url
        self.pages: List[Dict[str, Any]] = []
        self.issues: List[Dict[str, Any]] = []
        self.page_urls: Dict[str, str] = {}  # file_path -> url mapping
        
    def audit(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´ SEO å¯©è¨ˆ"""
        print("ğŸ” é–‹å§‹å…¨é¢ SEO å¯©è¨ˆ...\n")
        
        # 1. æƒææ‰€æœ‰é é¢
        print("ğŸ“ æƒæé é¢æ–‡ä»¶...")
        page_files = self._scan_pages()
        print(f"   æ‰¾åˆ° {len(page_files)} å€‹é é¢æ–‡ä»¶\n")
        
        # 2. è§£ææ‰€æœ‰é é¢ï¼ˆç¬¬ä¸€éï¼šå»ºç«‹URLæ˜ å°„ï¼‰
        print("ğŸ“„ è§£æé é¢ front matter...")
        for file_path in page_files:
            page_data = self._parse_page(file_path)
            if page_data:
                self.pages.append(page_data)
                self.page_urls[page_data['file_path']] = page_data['url']
        print(f"   æˆåŠŸè§£æ {len(self.pages)} å€‹é é¢\n")
        
        # 3. æ·±å…¥åˆ†ææ¯å€‹é é¢
        print("ğŸ” åŸ·è¡Œå…¨é¢ SEO åˆ†æ...")
        for page in self.pages:
            page['seo_analysis'] = self._comprehensive_analyze(page)
        print("   SEO åˆ†æå®Œæˆ\n")
        
        # 4. æª¢æŸ¥è·¨é é¢å•é¡Œ
        print("ğŸ”— æª¢æŸ¥è·¨é é¢å•é¡Œ...")
        self._check_cross_page_issues()
        print("   æª¢æŸ¥å®Œæˆ\n")
        
        # 5. ç”Ÿæˆçµ±è¨ˆ
        print("ğŸ“Š ç”Ÿæˆçµ±è¨ˆæ•¸æ“š...")
        stats = self._generate_stats()
        print("   çµ±è¨ˆå®Œæˆ\n")
        
        return {
            'timestamp': datetime.now().isoformat(),
            'site_url': self.site_url,
            'total_pages': len(self.pages),
            'pages': self.pages,
            'stats': stats,
            'issues': self.issues,
        }
    
    def _scan_pages(self) -> List[Path]:
        """æƒææ‰€æœ‰ .njk é é¢æ–‡ä»¶"""
        pages = []
        for file_path in self.src_dir.rglob("*.njk"):
            rel_path = file_path.relative_to(self.src_dir)
            if str(rel_path).startswith("_includes/"):
                continue
            if str(rel_path).startswith("_data/"):
                continue
            pages.append(file_path)
        return sorted(pages)
    
    def _parse_page(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """è§£æé é¢çš„ front matter å’Œå…§å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå– front matter
            front_matter_match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)$', content, re.DOTALL)
            
            if not front_matter_match:
                return None
            
            front_matter_text = front_matter_match.group(1)
            body_content = front_matter_match.group(2)
            
            # è§£æ YAML
            try:
                front_matter = self._parse_simple_yaml(front_matter_text)
            except Exception as e:
                print(f"   âš ï¸  YAML è§£æéŒ¯èª¤ ({file_path.relative_to(PROJECT_ROOT)}): {e}")
                return None
            
            # æ¨æ–· URL å’Œé é¢é¡å‹
            rel_path = file_path.relative_to(self.src_dir)
            url = self._infer_url(rel_path, front_matter)
            page_type = self._infer_page_type(rel_path, front_matter)
            
            return {
                'file_path': str(file_path.relative_to(PROJECT_ROOT)),
                'rel_path': str(rel_path),
                'url': url,
                'page_type': page_type,
                'front_matter': front_matter,
                'title': front_matter.get('title', ''),
                'seo': front_matter.get('seo', {}),
                'body_content': body_content,
                'content_length': len(body_content),
            }
        except Exception as e:
            print(f"   âš ï¸  è§£æéŒ¯èª¤ ({file_path.relative_to(PROJECT_ROOT)}): {e}")
            return None
    
    def _parse_simple_yaml(self, yaml_text: str) -> Dict[str, Any]:
        """ç°¡å–®çš„ YAML è§£æå™¨"""
        result = {}
        lines = yaml_text.split('\n')
        i = 0
        
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()
            
            if not stripped or stripped.startswith('#'):
                i += 1
                continue
            
            indent = len(line) - len(line.lstrip())
            
            if ':' in line:
                colon_idx = line.index(':')
                key = line[:colon_idx].strip()
                value_part = line[colon_idx + 1:].strip()
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯åµŒå¥—å°è±¡
                is_object = False
                if i + 1 < len(lines):
                    next_line = lines[i + 1]
                    next_stripped = next_line.strip()
                    if next_stripped and ':' in next_line:
                        next_indent = len(next_line) - len(next_line.lstrip())
                        if next_indent > indent:
                            is_object = True
                
                if is_object:
                    nested_lines = []
                    i += 1
                    while i < len(lines):
                        nested_line = lines[i]
                        nested_stripped = nested_line.strip()
                        if not nested_stripped or nested_stripped.startswith('#'):
                            i += 1
                            continue
                        nested_indent = len(nested_line) - len(nested_line.lstrip())
                        if nested_indent <= indent:
                            break
                        nested_lines.append(nested_line)
                        i += 1
                    i -= 1
                    
                    nested_text = '\n'.join(nested_lines)
                    result[key] = self._parse_simple_yaml(nested_text)
                else:
                    value = value_part
                    
                    if value in ('|', '>'):
                        multiline_lines = []
                        i += 1
                        while i < len(lines):
                            next_line = lines[i]
                            next_indent = len(next_line) - len(next_line.lstrip())
                            if next_indent <= indent and next_line.strip():
                                break
                            if next_indent > indent:
                                multiline_lines.append(next_line[indent:])
                            i += 1
                        value = '\n'.join(multiline_lines).strip()
                        if value.startswith('|') or value.startswith('>'):
                            value = value[1:].lstrip()
                        i -= 1
                    else:
                        if (value.startswith('"') and value.endswith('"')) or \
                           (value.startswith("'") and value.endswith("'")):
                            value = value[1:-1]
                    
                    result[key] = value
            
            i += 1
        
        return result
    
    def _infer_url(self, rel_path: Path, front_matter: Dict) -> str:
        """æ¨æ–·é é¢ URL"""
        if 'permalink' in front_matter:
            return front_matter['permalink']
        
        url_path = str(rel_path).replace('\\', '/')
        if url_path.endswith('.njk'):
            url_path = url_path[:-4]
        if url_path.endswith('/index'):
            url_path = url_path[:-6]
        elif url_path == 'index':
            url_path = ''
        
        url = f"{self.site_url}/{url_path}" if url_path else self.site_url
        return url.rstrip('/') + '/'
    
    def _infer_page_type(self, rel_path: Path, front_matter: Dict) -> str:
        """æ¨æ–·é é¢é¡å‹"""
        rel_str = str(rel_path).lower()
        if rel_str == 'index.njk':
            return 'home'
        elif 'services/' in rel_str:
            return 'service'
        elif 'blog/' in rel_str:
            return 'blog'
        elif 'guide/' in rel_str:
            return 'guide'
        elif 'booking/' in rel_str:
            return 'booking'
        elif 'about' in rel_str:
            return 'about'
        elif 'price-list' in rel_str or 'price' in rel_str:
            return 'pricing'
        else:
            return 'other'
    
    def _comprehensive_analyze(self, page: Dict[str, Any]) -> Dict[str, Any]:
        """å…¨é¢ SEO åˆ†æ"""
        analysis = {
            'scores': {},
            'overall_score': 0,
            'issues': [],
            'recommendations': [],
            'details': {},
        }
        
        body_content = page.get('body_content', '')
        
        # 1. Title åˆ†æ
        title_score, title_details = self._analyze_title(page.get('title', ''))
        analysis['scores']['title'] = title_score
        analysis['details']['title'] = title_details
        
        # 2. Description åˆ†æ
        description = page.get('seo', {}).get('description', '')
        desc_score, desc_details = self._analyze_description(description)
        analysis['scores']['description'] = desc_score
        analysis['details']['description'] = desc_details
        
        # 3. Keywords åˆ†æ
        keywords = page.get('seo', {}).get('keywords', '')
        keywords_score, keywords_details = self._analyze_keywords(keywords)
        analysis['scores']['keywords'] = keywords_score
        analysis['details']['keywords'] = keywords_details
        
        # 4. H1 åˆ†æ
        h1_score, h1_details = self._analyze_headings(body_content, 'h1')
        analysis['scores']['h1'] = h1_score
        analysis['details']['h1'] = h1_details
        
        # 5. æ¨™é¡Œçµæ§‹åˆ†æ (H1-H6)
        heading_structure_score, heading_details = self._analyze_heading_structure(body_content)
        analysis['scores']['heading_structure'] = heading_structure_score
        analysis['details']['heading_structure'] = heading_details
        
        # 6. å…§å®¹è³ªé‡åˆ†æ
        content_score, content_details = self._analyze_content_quality(body_content, page.get('title', ''))
        analysis['scores']['content'] = content_score
        analysis['details']['content'] = content_details
        
        # 7. åœ–ç‰‡åˆ†æ
        images_score, images_details = self._analyze_images(body_content)
        analysis['scores']['images'] = images_score
        analysis['details']['images'] = images_details
        
        # 8. å…§éƒ¨éˆæ¥åˆ†æ
        internal_links_score, links_details = self._analyze_internal_links(body_content, page['url'])
        analysis['scores']['internal_links'] = internal_links_score
        analysis['details']['internal_links'] = links_details
        
        # 9. å¤–éƒ¨éˆæ¥åˆ†æ
        external_links_score, external_details = self._analyze_external_links(body_content)
        analysis['scores']['external_links'] = external_links_score
        analysis['details']['external_links'] = external_details
        
        # 10. URL çµæ§‹åˆ†æ
        url_score, url_details = self._analyze_url(page['url'])
        analysis['scores']['url'] = url_score
        analysis['details']['url'] = url_details
        
        # 11. Meta æ¨™ç±¤å®Œæ•´æ€§ï¼ˆå¾ base-layout æª¢æŸ¥ï¼Œé€™è£¡åƒ…åšæç¤ºï¼‰
        meta_score, meta_details = self._analyze_meta_tags(page)
        analysis['scores']['meta_tags'] = meta_score
        analysis['details']['meta_tags'] = meta_details
        
        # 12. ç§»å‹•ç«¯å‹å¥½æ€§ï¼ˆæª¢æŸ¥ viewport metaï¼‰
        mobile_score, mobile_details = self._analyze_mobile_friendliness(page)
        analysis['scores']['mobile'] = mobile_score
        analysis['details']['mobile'] = mobile_details
        
        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        weights = {
            'title': 0.15,
            'description': 0.15,
            'keywords': 0.05,
            'h1': 0.10,
            'heading_structure': 0.08,
            'content': 0.15,
            'images': 0.10,
            'internal_links': 0.08,
            'external_links': 0.04,
            'url': 0.05,
            'meta_tags': 0.03,
            'mobile': 0.02,
        }
        
        overall_score = sum(
            analysis['scores'].get(key, 0) * weight
            for key, weight in weights.items()
        )
        
        analysis['overall_score'] = round(overall_score, 1)
        
        # æ”¶é›†æ‰€æœ‰å•é¡Œå’Œå»ºè­°
        for detail_key, detail_data in analysis['details'].items():
            if isinstance(detail_data, dict):
                if 'issues' in detail_data:
                    analysis['issues'].extend(detail_data['issues'])
                if 'recommendations' in detail_data:
                    analysis['recommendations'].extend(detail_data['recommendations'])
        
        return analysis
    
    def _analyze_title(self, title: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æ Title"""
        details = {
            'value': title,
            'length': len(title),
            'issues': [],
            'recommendations': [],
        }
        score = 0
        
        if not title:
            details['issues'].append({
                'priority': 'high',
                'message': 'ç¼ºå°‘ Title æ¨™ç±¤',
            })
            return 0, details
        
        std = SEO_STANDARDS['title']
        length = len(title)
        
        if length < std['min_length']:
            details['issues'].append({
                'priority': 'high',
                'message': f'Title éçŸ­ï¼ˆ{length} å­—ç¬¦ï¼Œå»ºè­°è‡³å°‘ {std["min_length"]} å­—ç¬¦ï¼‰',
            })
            score = (length / std['min_length']) * 50
        elif length > std['max_length']:
            details['issues'].append({
                'priority': 'medium',
                'message': f'Title éé•·ï¼ˆ{length} å­—ç¬¦ï¼Œå»ºè­°ä¸è¶…é {std["max_length"]} å­—ç¬¦ï¼‰',
            })
            excess = length - std['max_length']
            score = max(50, 100 - (excess * 2))
        else:
            if std['min_length'] <= length <= std['optimal_length']:
                score = 100
            else:
                score = 90
            details['recommendations'].append({
                'priority': 'low',
                'message': f'Title é•·åº¦è‰¯å¥½ï¼ˆ{length} å­—ç¬¦ï¼‰',
            })
        
        # æª¢æŸ¥å“ç‰Œå
        if 'å¥½æ™‚æœ‰å½±' not in title and 'Golden Years' not in title:
            details['recommendations'].append({
                'priority': 'low',
                'message': 'å»ºè­°åœ¨ Title ä¸­åŒ…å«å“ç‰Œåç¨±',
            })
            score *= 0.9
        
        return round(score, 1), details
    
    def _analyze_description(self, description: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æ Description"""
        details = {
            'value': description,
            'length': len(description),
            'issues': [],
            'recommendations': [],
        }
        score = 0
        
        if not description:
            details['issues'].append({
                'priority': 'high',
                'message': 'ç¼ºå°‘ Meta Description',
            })
            return 0, details
        
        std = SEO_STANDARDS['description']
        length = len(description)
        
        if length < std['min_length']:
            details['issues'].append({
                'priority': 'high',
                'message': f'Description éçŸ­ï¼ˆ{length} å­—ç¬¦ï¼Œå»ºè­°è‡³å°‘ {std["min_length"]} å­—ç¬¦ï¼‰',
            })
            score = (length / std['min_length']) * 60
        elif length > std['max_length']:
            details['issues'].append({
                'priority': 'medium',
                'message': f'Description éé•·ï¼ˆ{length} å­—ç¬¦ï¼Œå¯èƒ½è¢«æˆªæ–·ï¼‰',
            })
            excess = length - std['max_length']
            score = max(60, 100 - (excess * 2))
        else:
            score = 100
            details['recommendations'].append({
                'priority': 'low',
                'message': f'Description é•·åº¦è‰¯å¥½ï¼ˆ{length} å­—ç¬¦ï¼‰',
            })
        
        # æª¢æŸ¥ CTA
        cta_words = ['ç«‹å³', 'æŸ¥çœ‹', 'é ç´„', 'äº†è§£æ›´å¤š', 'é–‹å§‹']
        has_cta = any(word in description for word in cta_words)
        if not has_cta:
            details['recommendations'].append({
                'priority': 'low',
                'message': 'å»ºè­°åœ¨ Description ä¸­åŒ…å«è¡Œå‹•å‘¼ç±²ï¼ˆCTAï¼‰',
            })
            score *= 0.95
        
        return round(score, 1), details
    
    def _analyze_keywords(self, keywords: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æ Keywords"""
        details = {
            'value': keywords,
            'count': 0,
            'issues': [],
            'recommendations': [],
        }
        
        if not keywords:
            details['recommendations'].append({
                'priority': 'low',
                'message': 'å»ºè­°æ·»åŠ  keywordsï¼ˆé›–ç„¶ Google ä¸å†ä½¿ç”¨ï¼Œä½†å…¶ä»–æœç´¢å¼•æ“å¯èƒ½ä½¿ç”¨ï¼‰',
            })
            return 70, details
        
        if isinstance(keywords, str):
            keyword_list = [k.strip() for k in keywords.split(',') if k.strip()]
        elif isinstance(keywords, list):
            keyword_list = keywords
        else:
            keyword_list = []
        
        details['count'] = len(keyword_list)
        std = SEO_STANDARDS['keywords']
        
        if len(keyword_list) < std['min_count']:
            details['recommendations'].append({
                'priority': 'low',
                'message': f'é—œéµè©æ•¸é‡è¼ƒå°‘ï¼ˆ{len(keyword_list)} å€‹ï¼‰',
            })
            score = 80
        elif len(keyword_list) > std['max_count']:
            details['recommendations'].append({
                'priority': 'low',
                'message': f'é—œéµè©æ•¸é‡è¼ƒå¤šï¼ˆ{len(keyword_list)} å€‹ï¼Œå»ºè­°ä¸è¶…é {std["max_count"]} å€‹ï¼‰',
            })
            score = 85
        else:
            score = 100
        
        return round(score, 1), details
    
    def _analyze_headings(self, content: str, tag: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æç‰¹å®šæ¨™é¡Œæ¨™ç±¤"""
        pattern = f'<{tag}[^>]*>(.*?)</{tag}>'
        matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
        headings = [re.sub(r'<[^>]+>', '', h).strip() for h in matches]
        
        details = {
            'count': len(headings),
            'headings': headings,
            'issues': [],
            'recommendations': [],
        }
        
        if tag == 'h1':
            if len(headings) == 0:
                details['issues'].append({
                    'priority': 'high',
                    'message': 'é é¢ç¼ºå°‘ H1 æ¨™ç±¤',
                })
                return 0, details
            elif len(headings) == 1:
                details['recommendations'].append({
                    'priority': 'low',
                    'message': f'H1 æ¨™ç±¤è‰¯å¥½: "{headings[0]}"',
                })
                return 100, details
            else:
                details['issues'].append({
                    'priority': 'medium',
                    'message': f'é é¢åŒ…å«å¤šå€‹ H1 æ¨™ç±¤ï¼ˆ{len(headings)} å€‹ï¼‰ï¼Œå»ºè­°åªä½¿ç”¨ä¸€å€‹',
                })
                return 50, details
        
        return 100, details
    
    def _analyze_heading_structure(self, content: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†ææ¨™é¡Œçµæ§‹ï¼ˆH1-H6ï¼‰"""
        heading_counts = {f'h{i}': 0 for i in range(1, 7)}
        for i in range(1, 7):
            pattern = f'<h{i}[^>]*>'
            heading_counts[f'h{i}'] = len(re.findall(pattern, content, re.IGNORECASE))
        
        details = {
            'counts': heading_counts,
            'issues': [],
            'recommendations': [],
        }
        score = 100
        
        # æª¢æŸ¥ H1 æ•¸é‡
        if heading_counts['h1'] == 0:
            details['issues'].append({
                'priority': 'high',
                'message': 'ç¼ºå°‘ H1 æ¨™ç±¤',
            })
            score -= 30
        elif heading_counts['h1'] > 1:
            details['issues'].append({
                'priority': 'medium',
                'message': f'å¤šå€‹ H1 æ¨™ç±¤ï¼ˆ{heading_counts["h1"]} å€‹ï¼‰',
            })
            score -= 20
        
        # æª¢æŸ¥æ¨™é¡Œé †åºï¼ˆH1 å¾Œæ‡‰è©²æœ‰ H2ï¼‰
        if heading_counts['h1'] == 1 and heading_counts['h2'] == 0:
            details['recommendations'].append({
                'priority': 'low',
                'message': 'å»ºè­°åœ¨ H1 å¾Œä½¿ç”¨ H2 æ¨™é¡Œä¾†çµ„ç¹”å…§å®¹',
            })
            score -= 10
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è·³ç´šï¼ˆå¦‚ H1 å¾Œç›´æ¥ H3ï¼‰
        # é€™å€‹éœ€è¦æ›´è¤‡é›œçš„è§£æï¼Œæš«æ™‚è·³é
        
        total_headings = sum(heading_counts.values())
        if total_headings == 0:
            details['issues'].append({
                'priority': 'medium',
                'message': 'é é¢æ²’æœ‰ä»»ä½•æ¨™é¡Œæ¨™ç±¤ï¼Œå½±éŸ¿å…§å®¹çµæ§‹',
            })
            score -= 40
        elif total_headings < 3:
            details['recommendations'].append({
                'priority': 'low',
                'message': 'å»ºè­°ä½¿ç”¨æ›´å¤šæ¨™é¡Œæ¨™ç±¤ä¾†çµ„ç¹”å…§å®¹çµæ§‹',
            })
            score -= 5
        
        return max(0, round(score, 1)), details
    
    def _analyze_content_quality(self, content: str, title: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æå…§å®¹è³ªé‡"""
        # ç§»é™¤ HTML æ¨™ç±¤ç²å–ç´”æ–‡æœ¬
        text_content = re.sub(r'<[^>]+>', ' ', content)
        text_content = re.sub(r'\s+', ' ', text_content).strip()
        content_length = len(text_content)
        
        details = {
            'length': content_length,
            'word_count': len(text_content.split()),
            'issues': [],
            'recommendations': [],
        }
        score = 100
        
        std = SEO_STANDARDS['content']
        
        if content_length < std['min_length']:
            details['issues'].append({
                'priority': 'high',
                'message': f'å…§å®¹éçŸ­ï¼ˆ{content_length} å­—ç¬¦ï¼Œå»ºè­°è‡³å°‘ {std["min_length"]} å­—ç¬¦ï¼‰',
            })
            score = min(60, (content_length / std['min_length']) * 60)
        elif content_length < std['optimal_length']:
            details['recommendations'].append({
                'priority': 'low',
                'message': f'å…§å®¹é•·åº¦å¯æ¥å—ï¼ˆ{content_length} å­—ç¬¦ï¼‰ï¼Œä½†å»ºè­°é”åˆ° {std["optimal_length"]} å­—ç¬¦ä»¥ä¸Š',
            })
            score = 80
        else:
            details['recommendations'].append({
                'priority': 'low',
                'message': f'å…§å®¹é•·åº¦è‰¯å¥½ï¼ˆ{content_length} å­—ç¬¦ï¼‰',
            })
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¨™é¡Œç›¸é—œé—œéµè©
        if title:
            title_words = set(re.findall(r'\w+', title.lower()))
            content_words = set(re.findall(r'\w+', text_content.lower()))
            overlap = len(title_words & content_words)
            if overlap == 0 and len(title_words) > 0:
                details['recommendations'].append({
                    'priority': 'low',
                    'message': 'å»ºè­°å…§å®¹ä¸­åŒ…å«æ¨™é¡Œä¸­çš„é—œéµè©',
                })
                score *= 0.95
        
        return round(score, 1), details
    
    def _analyze_images(self, content: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æåœ–ç‰‡"""
        img_pattern = r'<img[^>]*>'
        images = re.findall(img_pattern, content, re.IGNORECASE)
        
        total_images = len(images)
        images_with_alt = 0
        images_without_alt = []
        
        for img_tag in images:
            alt_match = re.search(r'alt=["\']([^"\']*)["\']', img_tag, re.IGNORECASE)
            if alt_match and alt_match.group(1).strip():
                images_with_alt += 1
            else:
                # æå– src ç”¨æ–¼å ±å‘Š
                src_match = re.search(r'src=["\']([^"\']*)["\']', img_tag, re.IGNORECASE)
                src = src_match.group(1) if src_match else 'æœªçŸ¥'
                images_without_alt.append(src)
        
        details = {
            'total': total_images,
            'with_alt': images_with_alt,
            'without_alt': len(images_without_alt),
            'missing_alt_images': images_without_alt[:5],  # åªè¨˜éŒ„å‰5å€‹
            'issues': [],
            'recommendations': [],
        }
        
        if total_images == 0:
            details['recommendations'].append({
                'priority': 'low',
                'message': 'é é¢æ²’æœ‰åœ–ç‰‡ï¼Œå¯ä»¥è€ƒæ…®æ·»åŠ ç›¸é—œåœ–ç‰‡æå‡ç”¨æˆ¶é«”é©—',
            })
            return 80, details
        
        if images_without_alt:
            details['issues'].append({
                'priority': 'high',
                'message': f'{len(images_without_alt)} å¼µåœ–ç‰‡ç¼ºå°‘ alt å±¬æ€§',
            })
            score = (images_with_alt / total_images) * 100
        else:
            details['recommendations'].append({
                'priority': 'low',
                'message': 'æ‰€æœ‰åœ–ç‰‡éƒ½åŒ…å« alt å±¬æ€§ï¼Œè‰¯å¥½ï¼',
            })
            score = 100
        
        return round(score, 1), details
    
    def _analyze_internal_links(self, content: str, current_url: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æå…§éƒ¨éˆæ¥"""
        link_pattern = r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>'
        links = re.findall(link_pattern, content, re.IGNORECASE)
        
        internal_links = []
        external_links = []
        
        for link in links:
            if link.startswith('http://') or link.startswith('https://'):
                if self.site_url in link:
                    internal_links.append(link)
                else:
                    external_links.append(link)
            elif link.startswith('/') or not link.startswith('#'):
                internal_links.append(link)
        
        details = {
            'total': len(internal_links),
            'links': internal_links[:10],  # åªè¨˜éŒ„å‰10å€‹
            'issues': [],
            'recommendations': [],
        }
        score = 100
        
        std = SEO_STANDARDS['links']
        
        if len(internal_links) < std['min_internal_links']:
            details['issues'].append({
                'priority': 'medium',
                'message': f'å…§éƒ¨éˆæ¥è¼ƒå°‘ï¼ˆ{len(internal_links)} å€‹ï¼Œå»ºè­°è‡³å°‘ {std["min_internal_links"]} å€‹ï¼‰',
            })
            score = min(70, (len(internal_links) / std['min_internal_links']) * 70)
        elif len(internal_links) >= std['min_internal_links']:
            details['recommendations'].append({
                'priority': 'low',
                'message': f'å…§éƒ¨éˆæ¥æ•¸é‡è‰¯å¥½ï¼ˆ{len(internal_links)} å€‹ï¼‰',
            })
        
        return round(score, 1), details
    
    def _analyze_external_links(self, content: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æå¤–éƒ¨éˆæ¥"""
        link_pattern = r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>'
        links = re.findall(link_pattern, content, re.IGNORECASE)
        
        external_links = []
        for link in links:
            if (link.startswith('http://') or link.startswith('https://')) and self.site_url not in link:
                external_links.append(link)
        
        details = {
            'total': len(external_links),
            'links': external_links[:5],
            'issues': [],
            'recommendations': [],
        }
        score = 100
        
        std = SEO_STANDARDS['links']
        
        if len(external_links) > std['max_external_links']:
            details['issues'].append({
                'priority': 'low',
                'message': f'å¤–éƒ¨éˆæ¥è¼ƒå¤šï¼ˆ{len(external_links)} å€‹ï¼‰ï¼Œå¯èƒ½å½±éŸ¿é é¢æ¬Šé‡å‚³é',
            })
            score = max(80, 100 - (len(external_links) - std['max_external_links']) * 2)
        elif len(external_links) > 0:
            details['recommendations'].append({
                'priority': 'low',
                'message': f'å¤–éƒ¨éˆæ¥æ•¸é‡åˆç†ï¼ˆ{len(external_links)} å€‹ï¼‰ï¼Œå»ºè­°æ·»åŠ  rel="nofollow" å±¬æ€§',
            })
        
        return round(score, 1), details
    
    def _analyze_url(self, url: str) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æ URL çµæ§‹"""
        details = {
            'url': url,
            'length': len(url),
            'issues': [],
            'recommendations': [],
        }
        score = 100
        
        # æª¢æŸ¥é•·åº¦
        if len(url) > 100:
            details['issues'].append({
                'priority': 'medium',
                'message': f'URL éé•·ï¼ˆ{len(url)} å­—ç¬¦ï¼‰',
            })
            score -= 20
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«åƒæ•¸
        if '?' in url or '&' in url:
            details['recommendations'].append({
                'priority': 'low',
                'message': 'URL åŒ…å«æŸ¥è©¢åƒæ•¸ï¼Œå»ºè­°ä½¿ç”¨å‹å¥½çš„ URL çµæ§‹',
            })
            score -= 10
        
        # æª¢æŸ¥æ·±åº¦ï¼ˆæ–œç·šæ•¸é‡ï¼‰
        depth = url.count('/') - 3  # æ¸›å» http://domain.com/
        if depth > 4:
            details['recommendations'].append({
                'priority': 'low',
                'message': f'URL æ·±åº¦è¼ƒæ·±ï¼ˆ{depth} å±¤ï¼‰ï¼Œå»ºè­°ä¿æŒæ·ºå±¤çµæ§‹',
            })
            score -= 5
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµè©ï¼ˆç›¸å° URL éƒ¨åˆ†ï¼‰
        url_path = url.replace(self.site_url, '').strip('/')
        if not url_path or url_path in ['', '/']:
            # é¦–é ï¼Œä¸éœ€è¦æª¢æŸ¥
            pass
        elif re.match(r'^[a-z0-9\-/]+$', url_path, re.IGNORECASE):
            details['recommendations'].append({
                'priority': 'low',
                'message': 'URL çµæ§‹æ¸…æ™°ï¼Œä½¿ç”¨å°å¯«å­—æ¯å’Œé€£å­—ç¬¦',
            })
        
        return max(0, round(score, 1)), details
    
    def _analyze_meta_tags(self, page: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æ Meta æ¨™ç±¤å®Œæ•´æ€§"""
        details = {
            'has_description': bool(page.get('seo', {}).get('description')),
            'has_keywords': bool(page.get('seo', {}).get('keywords')),
            'issues': [],
            'recommendations': [],
        }
        score = 100
        
        # æª¢æŸ¥ Open Graphï¼ˆéœ€è¦åœ¨æ¨¡æ¿ä¸­æª¢æŸ¥ï¼Œé€™è£¡åƒ…æç¤ºï¼‰
        details['recommendations'].append({
            'priority': 'low',
            'message': 'å»ºè­°æ·»åŠ  Open Graph æ¨™ç±¤ä»¥æ”¹å–„ç¤¾äº¤åª’é«”åˆ†äº«æ•ˆæœ',
        })
        score -= 10
        
        # æª¢æŸ¥ Twitter Cards
        details['recommendations'].append({
            'priority': 'low',
            'message': 'å»ºè­°æ·»åŠ  Twitter Card æ¨™ç±¤',
        })
        score -= 10
        
        return round(score, 1), details
    
    def _analyze_mobile_friendliness(self, page: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """åˆ†æç§»å‹•ç«¯å‹å¥½æ€§"""
        # æª¢æŸ¥ viewport metaï¼ˆé€šå¸¸åœ¨ base-layout ä¸­ï¼Œé€™è£¡å‡è¨­æœ‰ï¼‰
        details = {
            'viewport_expected': True,  # base-layout.njk ä¸­æ‡‰è©²æœ‰
            'issues': [],
            'recommendations': [],
        }
        score = 100
        
        details['recommendations'].append({
            'priority': 'low',
            'message': 'å»ºè­°ç¢ºä¿ viewport meta æ¨™ç±¤å·²è¨­ç½®ï¼ˆé€šå¸¸å·²åœ¨ base-layout ä¸­ï¼‰',
        })
        
        return score, details
    
    def _check_cross_page_issues(self):
        """æª¢æŸ¥è·¨é é¢å•é¡Œ"""
        # æª¢æŸ¥é‡è¤‡çš„ title
        titles = defaultdict(list)
        descriptions = defaultdict(list)
        
        for page in self.pages:
            title = page.get('title', '')
            if title:
                titles[title].append(page['url'])
            
            description = page.get('seo', {}).get('description', '')
            if description:
                descriptions[description].append(page['url'])
        
        # é‡è¤‡ title
        for title, urls in titles.items():
            if len(urls) > 1:
                self.issues.append({
                    'type': 'duplicate_title',
                    'priority': 'high',
                    'message': f'æ¨™é¡Œé‡è¤‡: "{title}"',
                    'affected_pages': urls[:5],
                    'count': len(urls),
                })
        
        # é‡è¤‡ description
        for desc, urls in descriptions.items():
            if len(urls) > 1:
                self.issues.append({
                    'type': 'duplicate_description',
                    'priority': 'medium',
                    'message': f'æè¿°é‡è¤‡ï¼ˆå‡ºç¾åœ¨ {len(urls)} å€‹é é¢ï¼‰',
                    'affected_pages': urls[:3],
                    'count': len(urls),
                })
    
    def _generate_stats(self) -> Dict[str, Any]:
        """ç”Ÿæˆçµ±è¨ˆæ•¸æ“š"""
        if not self.pages:
            return {}
        
        scores = [p['seo_analysis']['overall_score'] for p in self.pages]
        all_scores = {key: [] for key in ['title', 'description', 'keywords', 'h1', 'heading_structure', 
                                          'content', 'images', 'internal_links', 'external_links', 'url', 
                                          'meta_tags', 'mobile']}
        
        for page in self.pages:
            for key in all_scores:
                if key in page['seo_analysis']['scores']:
                    all_scores[key].append(page['seo_analysis']['scores'][key])
        
        category_averages = {
            key: round(sum(values) / len(values), 1) if values else 0
            for key, values in all_scores.items()
        }
        
        # çµ±è¨ˆå•é¡Œ
        total_issues = sum(len(p['seo_analysis']['issues']) for p in self.pages)
        pages_with_issues = sum(1 for p in self.pages if p['seo_analysis']['issues'])
        
        # å•é¡Œå„ªå…ˆç´šçµ±è¨ˆ
        issue_priority_count = {'high': 0, 'medium': 0, 'low': 0}
        for page in self.pages:
            for issue in page['seo_analysis']['issues']:
                priority = issue.get('priority', 'low')
                issue_priority_count[priority] = issue_priority_count.get(priority, 0) + 1
        
        return {
            'overall': {
                'average_score': round(sum(scores) / len(scores), 1),
                'min_score': round(min(scores), 1),
                'max_score': round(max(scores), 1),
                'median_score': round(sorted(scores)[len(scores) // 2], 1) if scores else 0,
            },
            'category_averages': category_averages,
            'pages_with_issues': pages_with_issues,
            'total_issues': total_issues,
            'issue_priority_count': issue_priority_count,
            'total_pages': len(self.pages),
        }


def generate_detailed_report(audit_result: Dict[str, Any]) -> str:
    """ç”Ÿæˆè©³ç´°çš„ Markdown å ±å‘Š"""
    lines = []
    
    lines.append("# ğŸ“Š å…¨é¢ SEO å¯©è¨ˆå ±å‘Š")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ™‚é–“**: {audit_result['timestamp']}")
    lines.append(f"**ç¶²ç«™ URL**: {audit_result['site_url']}")
    lines.append(f"**ç¸½é é¢æ•¸**: {audit_result['total_pages']}")
    lines.append("")
    
    # ç¸½é«”çµ±è¨ˆ
    stats = audit_result['stats']
    overall_stats = stats.get('overall', {})
    
    lines.append("## ğŸ“ˆ ç¸½é«”çµ±è¨ˆ")
    lines.append("")
    lines.append(f"- **å¹³å‡ SEO åˆ†æ•¸**: {overall_stats.get('average_score', 0)}/100")
    lines.append(f"- **æœ€ä½åˆ†æ•¸**: {overall_stats.get('min_score', 0)}/100")
    lines.append(f"- **æœ€é«˜åˆ†æ•¸**: {overall_stats.get('max_score', 0)}/100")
    lines.append(f"- **ä¸­ä½æ•¸åˆ†æ•¸**: {overall_stats.get('median_score', 0)}/100")
    lines.append(f"- **æœ‰å•é¡Œçš„é é¢**: {stats.get('pages_with_issues', 0)}/{audit_result['total_pages']}")
    lines.append(f"- **ç¸½å•é¡Œæ•¸**: {stats.get('total_issues', 0)}")
    lines.append("")
    
    # åˆ†é¡å¹³å‡åˆ†æ•¸
    category_averages = stats.get('category_averages', {})
    if category_averages:
        lines.append("### åˆ†é¡å¹³å‡åˆ†æ•¸")
        lines.append("")
        category_names = {
            'title': 'æ¨™é¡Œ (Title)',
            'description': 'æè¿° (Description)',
            'keywords': 'é—œéµè© (Keywords)',
            'h1': 'H1 æ¨™ç±¤',
            'heading_structure': 'æ¨™é¡Œçµæ§‹',
            'content': 'å…§å®¹è³ªé‡',
            'images': 'åœ–ç‰‡å„ªåŒ–',
            'internal_links': 'å…§éƒ¨éˆæ¥',
            'external_links': 'å¤–éƒ¨éˆæ¥',
            'url': 'URL çµæ§‹',
            'meta_tags': 'Meta æ¨™ç±¤',
            'mobile': 'ç§»å‹•ç«¯å‹å¥½',
        }
        
        for key, avg_score in sorted(category_averages.items(), key=lambda x: x[1]):
            name = category_names.get(key, key)
            lines.append(f"- **{name}**: {avg_score}/100")
        lines.append("")
    
    # å•é¡Œå„ªå…ˆç´šçµ±è¨ˆ
    issue_priority = stats.get('issue_priority_count', {})
    if issue_priority:
        lines.append("### å•é¡Œå„ªå…ˆç´šçµ±è¨ˆ")
        lines.append("")
        lines.append(f"- ğŸ”´ **é«˜å„ªå…ˆç´š**: {issue_priority.get('high', 0)} å€‹")
        lines.append(f"- ğŸŸ¡ **ä¸­å„ªå…ˆç´š**: {issue_priority.get('medium', 0)} å€‹")
        lines.append(f"- ğŸŸ¢ **ä½å„ªå…ˆç´š**: {issue_priority.get('low', 0)} å€‹")
        lines.append("")
    
    # é é¢è©³æƒ…
    lines.append("## ğŸ“„ é é¢è©³æƒ…")
    lines.append("")
    lines.append("æŒ‰ SEO åˆ†æ•¸æ’åºï¼ˆå¾ä½åˆ°é«˜ï¼‰")
    lines.append("")
    
    sorted_pages = sorted(audit_result['pages'], key=lambda x: x['seo_analysis']['overall_score'])
    
    for page in sorted_pages:
        url = page['url']
        title = page.get('title', 'ç„¡æ¨™é¡Œ')
        score = page['seo_analysis']['overall_score']
        analysis = page['seo_analysis']
        
        # åˆ†æ•¸é¡è‰²æ¨™è¨˜
        if score >= 80:
            score_badge = f"ğŸŸ¢ {score}"
        elif score >= 60:
            score_badge = f"ğŸŸ¡ {score}"
        else:
            score_badge = f"ğŸ”´ {score}"
        
        lines.append(f"### {title} {score_badge}")
        lines.append("")
        lines.append(f"- **URL**: {url}")
        lines.append(f"- **æ–‡ä»¶**: `{page['file_path']}`")
        lines.append(f"- **é é¢é¡å‹**: {page.get('page_type', 'unknown')}")
        lines.append("")
        
        # å„é …åˆ†æ•¸è©³æƒ…
        lines.append("#### ğŸ“Š åˆ†é …è©•åˆ†")
        lines.append("")
        score_names = {
            'title': 'æ¨™é¡Œ (Title)',
            'description': 'æè¿° (Description)',
            'keywords': 'é—œéµè©',
            'h1': 'H1 æ¨™ç±¤',
            'heading_structure': 'æ¨™é¡Œçµæ§‹',
            'content': 'å…§å®¹è³ªé‡',
            'images': 'åœ–ç‰‡å„ªåŒ–',
            'internal_links': 'å…§éƒ¨éˆæ¥',
            'external_links': 'å¤–éƒ¨éˆæ¥',
            'url': 'URL çµæ§‹',
            'meta_tags': 'Meta æ¨™ç±¤',
            'mobile': 'ç§»å‹•ç«¯å‹å¥½',
        }
        
        for key, name in score_names.items():
            if key in analysis['scores']:
                score_val = analysis['scores'][key]
                detail = analysis['details'].get(key, {})
                
                # æ ¹æ“šé¡åˆ¥é¡¯ç¤ºé¡å¤–ä¿¡æ¯
                if key == 'title' and 'length' in detail:
                    lines.append(f"- **{name}**: {score_val}/100 ({detail['length']} å­—ç¬¦)")
                elif key == 'description' and 'length' in detail:
                    lines.append(f"- **{name}**: {score_val}/100 ({detail['length']} å­—ç¬¦)")
                elif key == 'h1' and 'count' in detail:
                    lines.append(f"- **{name}**: {score_val}/100 ({detail['count']} å€‹)")
                elif key == 'content' and 'length' in detail:
                    lines.append(f"- **{name}**: {score_val}/100 ({detail['length']} å­—ç¬¦)")
                elif key == 'images' and 'total' in detail:
                    lines.append(f"- **{name}**: {score_val}/100 ({detail['total']} å¼µåœ–ç‰‡, {detail.get('with_alt', 0)} å¼µæœ‰ alt)")
                elif key == 'internal_links' and 'total' in detail:
                    lines.append(f"- **{name}**: {score_val}/100 ({detail['total']} å€‹å…§éƒ¨éˆæ¥)")
                else:
                    lines.append(f"- **{name}**: {score_val}/100")
        
        lines.append("")
        
        # å•é¡Œ
        if analysis['issues']:
            lines.append("#### âš ï¸ å•é¡Œ")
            lines.append("")
            # æŒ‰å„ªå…ˆç´šæ’åº
            sorted_issues = sorted(analysis['issues'], key=lambda x: {'high': 0, 'medium': 1, 'low': 2}.get(x.get('priority', 'low'), 3))
            for issue in sorted_issues:
                priority = issue.get('priority', 'low')
                priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(priority, 'âšª')
                lines.append(f"- {priority_emoji} **{priority.upper()}**: {issue['message']}")
            lines.append("")
        
        # å»ºè­°
        if analysis['recommendations']:
            lines.append("#### ğŸ’¡ å»ºè­°")
            lines.append("")
            sorted_recs = sorted(analysis['recommendations'], key=lambda x: {'high': 0, 'medium': 1, 'low': 2}.get(x.get('priority', 'low'), 3))
            for rec in sorted_recs:
                priority = rec.get('priority', 'low')
                priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(priority, 'âšª')
                lines.append(f"- {priority_emoji} **{priority.upper()}**: {rec['message']}")
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    # å…¨å±€å•é¡Œ
    if audit_result['issues']:
        lines.append("## ğŸ”— å…¨å±€å•é¡Œ")
        lines.append("")
        for issue in audit_result['issues']:
            priority = issue.get('priority', 'low')
            priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(priority, 'âšª')
            lines.append(f"### {priority_emoji} {issue['message']}")
            lines.append("")
            lines.append(f"**å½±éŸ¿é é¢æ•¸**: {issue.get('count', 0)}")
            lines.append("")
            lines.append("**å—å½±éŸ¿çš„é é¢**:")
            for url in issue.get('affected_pages', [])[:5]:
                lines.append(f"- {url}")
            if issue.get('count', 0) > 5:
                lines.append(f"- ... é‚„æœ‰ {issue['count'] - 5} å€‹é é¢")
            lines.append("")
    
    return '\n'.join(lines)


def main():
    """ä¸»å‡½æ•¸"""
    # è®€å–ç¶²ç«™ URL
    metadata_file = PROJECT_ROOT / "src" / "_data" / "metadata.json"
    site_url = SITE_URL
    if metadata_file.exists():
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                site_url = metadata.get('url', SITE_URL)
        except:
            pass
    
    # ç¢ºä¿å ±å‘Šç›®éŒ„å­˜åœ¨
    REPORT_DIR.mkdir(exist_ok=True)
    
    # åŸ·è¡Œå¯©è¨ˆ
    auditor = ComprehensiveSEOAuditor(SRC_DIR, site_url)
    audit_result = auditor.audit()
    
    # ä¿å­˜ JSON å ±å‘Š
    json_report_path = REPORT_DIR / "comprehensive-seo-audit.json"
    with open(json_report_path, 'w', encoding='utf-8') as f:
        json.dump(audit_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON å ±å‘Šå·²ä¿å­˜: {json_report_path}")
    
    # ç”Ÿæˆä¸¦ä¿å­˜ Markdown å ±å‘Š
    md_report = generate_detailed_report(audit_result)
    md_report_path = REPORT_DIR / "comprehensive-seo-audit.md"
    with open(md_report_path, 'w', encoding='utf-8') as f:
        f.write(md_report)
    print(f"âœ… Markdown å ±å‘Šå·²ä¿å­˜: {md_report_path}")
    
    # è¼¸å‡ºæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š å…¨é¢ SEO å¯©è¨ˆæ‘˜è¦")
    print("="*60)
    stats = audit_result['stats']
    overall = stats.get('overall', {})
    print(f"ç¸½é é¢æ•¸: {audit_result['total_pages']}")
    print(f"å¹³å‡åˆ†æ•¸: {overall.get('average_score', 0)}/100")
    print(f"æœ€ä½åˆ†æ•¸: {overall.get('min_score', 0)}/100")
    print(f"æœ€é«˜åˆ†æ•¸: {overall.get('max_score', 0)}/100")
    print(f"æœ‰å•é¡Œçš„é é¢: {stats.get('pages_with_issues', 0)}/{audit_result['total_pages']}")
    print(f"ç¸½å•é¡Œæ•¸: {stats.get('total_issues', 0)}")
    
    # é¡¯ç¤ºæœ€éœ€è¦æ”¹é€²çš„å‰5å€‹é é¢
    sorted_pages = sorted(audit_result['pages'], key=lambda x: x['seo_analysis']['overall_score'])
    print("\næœ€éœ€è¦æ”¹é€²çš„5å€‹é é¢:")
    for i, page in enumerate(sorted_pages[:5], 1):
        print(f"  {i}. {page.get('title', 'ç„¡æ¨™é¡Œ')[:40]} - {page['seo_analysis']['overall_score']}/100")
    
    print("="*60)


if __name__ == '__main__':
    main()
