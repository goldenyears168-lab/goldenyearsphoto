#!/usr/bin/env python3
"""
SEO å„ªåŒ–è…³æœ¬
æ ¹æ“š SEO æ”¹å–„æ–¹æ¡ˆè‡ªå‹•å„ªåŒ–é é¢çš„ front matter
"""

import re
from pathlib import Path
from typing import Dict, Tuple, Optional

PROJECT_ROOT = Path(__file__).parent.parent
SRC_DIR = PROJECT_ROOT / "src"

# SEO å„ªåŒ–å»ºè­°ï¼ˆæ ¹æ“šé é¢é¡å‹ï¼‰
SEO_IMPROVEMENTS = {
    'index.njk': {
        'title': 'å°åŒ—å°ˆæ¥­å½¢è±¡ç…§ï½œéŸ“å¼è­‰ä»¶ç…§ã€å±¥æ­·ç…§æœå‹™ - å¥½æ™‚æœ‰å½± Golden Years',
        'description': 'å°åŒ—å°ˆæ¥­å½¢è±¡ç…§æ”å½±é¦–é¸ï½œéŸ“å¼è­‰ä»¶ç…§ã€å±¥æ­·ç…§ã€ç•¢æ¥­ç…§æœå‹™ã€‚å¥½æ™‚æœ‰å½±ä½æ–¼ä¸­å±±/å…¬é¤¨ï¼Œç‚ºé†«å¸«ã€å¾‹å¸«ã€ä¼æ¥­ä¸»æ‰“é€ è·å ´ç¬¬ä¸€å°è±¡ã€‚ç«‹å³é ç´„æ‹æ”ï¼ŒæŸ¥çœ‹åƒ¹ç›®è¡¨èˆ‡ä½œå“é›†ã€‚',
        'keywords': 'éŸ“å¼è­‰ä»¶ç…§, å°ˆæ¥­å½¢è±¡ç…§, å±¥æ­·ç…§, ç•¢æ¥­ç…§, LinkedIné ­åƒ, å°åŒ—æ”å½±, ä¸­å±±æ”å½±, å…¬é¤¨æ”å½±, å½¢è±¡ç…§æ¨è–¦',
    },
    'services/portrait.njk': {
        'title': 'å°ˆæ¥­å½¢è±¡ç…§æ¨è–¦ï½œLinkedInå±¥æ­·ç…§ã€é†«å¸«å¾‹å¸«å½¢è±¡ç…§ - å¥½æ™‚æœ‰å½±',
        'description': 'å°ˆæ¥­å½¢è±¡ç…§ä¸èƒ½ç”¨AIç”Ÿæˆï¼Œæ‚¨çš„è·å ´ç¬¬ä¸€å°è±¡éœ€è¦çœŸå¯¦å‘ˆç¾ã€‚å¥½æ™‚æœ‰å½±ç‚ºé†«å¸«ã€å¾‹å¸«ã€è¬›å¸«ã€ä¼æ¥­ä¸»æä¾›LinkedInå±¥æ­·ç…§æœå‹™ï¼Œä½æ–¼å°åŒ—ä¸­å±±/å…¬é¤¨ï¼Œç«‹å³é ç´„æ‹æ”ï¼Œæ‰“é€ å°ˆæ¥­å€‹äººå“ç‰Œå½¢è±¡ã€‚',
        'keywords': 'å°åŒ—å°ˆæ¥­å½¢è±¡ç…§æ¨è–¦, å°ˆæ¥­å½¢è±¡ç…§, LinkedIné ­åƒ, å±¥æ­·ç…§, é†«å¸«å½¢è±¡ç…§, å¾‹å¸«å½¢è±¡ç…§, æ±‚è·å½¢è±¡ç…§, å€‹äººå“ç‰Œæ”å½±, è·å ´äººåƒå¯«çœŸ',
    },
    'about.njk': {
        'title': 'é—œæ–¼æˆ‘å€‘ï½œå°ˆæ¥­æ”å½±åœ˜éšŠä»‹ç´¹ - å¥½æ™‚æœ‰å½± Golden Years',
        'description': 'èªè­˜å¥½æ™‚æœ‰å½±å°ˆæ¥­æ”å½±åœ˜éšŠï¼šç¶“é©—è±å¯Œçš„æ”å½±å¸«ã€å°ˆæ¥­é€ å‹å¸«èˆ‡ä¿®åœ–å¸«ï¼Œç‚ºæ‚¨æ‰“é€ çœŸå¯¦åˆå¥½çœ‹çš„è·å ´å½¢è±¡ç…§ã€‚æˆ‘å€‘ç›¸ä¿¡ä¸€å¼µå¥½çš„å½¢è±¡ç…§æ˜¯è·æ¶¯è‡ªä¿¡çš„èµ·é»ï¼Œç«‹å³äº†è§£æˆ‘å€‘çš„æœå‹™ç†å¿µèˆ‡åœ˜éšŠæ•…äº‹ã€‚',
        'keywords': 'å¥½æ™‚æœ‰å½±åœ˜éšŠ, æ”å½±å¸«ä»‹ç´¹, é€ å‹å¸«ä»‹ç´¹, ä¿®åœ–å¸«, å°ˆæ¥­æ”å½±åœ˜éšŠ, å°åŒ—ç…§ç›¸é¤¨, å½¢è±¡ç…§æ”å½±å¸«æ¨è–¦',
    },
    'price-list.njk': {
        'title': 'åƒ¹ç›®è¡¨ï½œéŸ“å¼è­‰ä»¶ç…§ã€å°ˆæ¥­å½¢è±¡ç…§åƒ¹æ ¼ - å¥½æ™‚æœ‰å½±',
        'description': 'å¥½æ™‚æœ‰å½±åƒ¹æ ¼é€æ˜å…¬é–‹ï¼ŒéŸ“å¼è­‰ä»¶ç…§ã€å°ˆæ¥­å½¢è±¡ç…§å®Œæ•´åƒ¹ç›®è¡¨ã€‚æä¾›æ€¥ä»¶æœå‹™ï¼Œæ‰€æœ‰åƒ¹æ ¼å«å¦é«®èˆ‡ç²¾ä¿®ï¼Œç«‹å³æŸ¥çœ‹å„é …æœå‹™åƒ¹æ ¼ï¼Œé¸æ“‡æœ€é©åˆçš„æ‹æ”æ–¹æ¡ˆã€‚',
        'keywords': 'å¥½æ™‚æœ‰å½±åƒ¹ç›®è¡¨, è­‰ä»¶ç…§åƒ¹æ ¼, å½¢è±¡ç…§åƒ¹æ ¼, éŸ“å¼è­‰ä»¶ç…§åƒ¹æ ¼, å°ˆæ¥­å½¢è±¡ç…§è²»ç”¨, å°åŒ—æ”å½±åƒ¹æ ¼',
    },
    'booking/index.njk': {
        'title': 'ç«‹å³é ç´„ï½œä¸­å±±åº—/å…¬é¤¨åº—å°ˆæ¥­å½¢è±¡ç…§æ‹æ” - å¥½æ™‚æœ‰å½±',
        'description': 'å¥½æ™‚æœ‰å½±é ç´„ç³»çµ±ï¼Œé¸æ“‡ä¸­å±±åº—æˆ–å…¬é¤¨åº—é€²è¡Œå°ˆæ¥­å½¢è±¡ç…§æ‹æ”ã€‚ç·šä¸Šé ç´„æµç¨‹ç°¡å–®ï¼Œæä¾›å®Œæ•´æœå‹™èªªæ˜èˆ‡æ³¨æ„äº‹é …ï¼Œç«‹å³é¸æ“‡é©åˆçš„æ™‚é–“åœ°é»ï¼Œé–‹å§‹æ‚¨çš„è·å ´å½¢è±¡æ‰“é€ ä¹‹æ—…ã€‚',
        'keywords': 'å¥½æ™‚æœ‰å½±é ç´„, ä¸­å±±åº—é ç´„, å…¬é¤¨åº—é ç´„, å½¢è±¡ç…§é ç´„, è­‰ä»¶ç…§é ç´„, å°åŒ—æ”å½±é ç´„',
    },
    'booking/zhongshan.njk': {
        'title': 'é ç´„ä¸­å±±åº—ï½œå°åŒ—å°ˆæ¥­å½¢è±¡ç…§æ‹æ”æœå‹™ - å¥½æ™‚æœ‰å½±',
        'description': 'å¥½æ™‚æœ‰å½±ä¸­å±±åº—ä½æ–¼æ·é‹ä¸­å±±ç«™2è™Ÿå‡ºå£ï¼Œæä¾›éŸ“å¼è­‰ä»¶ç…§ã€å°ˆæ¥­å½¢è±¡ç…§æœå‹™ã€‚äº¤é€šä¾¿åˆ©ï¼Œå°ˆæ¥­åœ˜éšŠç‚ºæ‚¨æ‰“é€ è·å ´ç¬¬ä¸€å°è±¡ï¼Œç«‹å³é ç´„æ‹æ”æ™‚é–“ï¼Œé«”é©—å°ˆæ¥­æ”å½±æœå‹™ã€‚',
        'keywords': 'å¥½æ™‚æœ‰å½±ä¸­å±±åº—, ä¸­å±±ç«™æ”å½±, å°åŒ—ä¸­å±±å½¢è±¡ç…§, ä¸­å±±å€è­‰ä»¶ç…§, å¥½æ™‚æœ‰å½±é ç´„ä¸­å±±',
    },
    'booking/gongguan.njk': {
        'title': 'é ç´„å…¬é¤¨åº—ï½œå°åŒ—å°ˆæ¥­å½¢è±¡ç…§æ‹æ”æœå‹™ - å¥½æ™‚æœ‰å½±',
        'description': 'å¥½æ™‚æœ‰å½±å…¬é¤¨åº—ä½æ–¼æ·é‹å…¬é¤¨ç«™1è™Ÿå‡ºå£ï¼Œæä¾›éŸ“å¼è­‰ä»¶ç…§ã€å°ˆæ¥­å½¢è±¡ç…§æœå‹™ã€‚é„°è¿‘å°å¤§ã€å¸«å¤§ï¼Œæ–¹ä¾¿å­¸ç”Ÿèˆ‡è·å ´äººå£«é ç´„ï¼Œç«‹å³é ç´„æ‹æ”æ™‚é–“ï¼Œæ‰“é€ å°ˆæ¥­å€‹äººå½¢è±¡ã€‚',
        'keywords': 'å¥½æ™‚æœ‰å½±å…¬é¤¨åº—, å…¬é¤¨ç«™æ”å½±, å°åŒ—å…¬é¤¨å½¢è±¡ç…§, å…¬é¤¨å€è­‰ä»¶ç…§, å¥½æ™‚æœ‰å½±é ç´„å…¬é¤¨, å°å¤§æ”å½±',
    },
    'guide/identity-test.njk': {
        'title': 'èº«ä»½åŸå‹æ¸¬é©—ï½œæ‰¾å‡ºä½ çš„è·å ´äººæ ¼é¡å‹ - å¥½æ™‚æœ‰å½±',
        'description': 'é€éRIASECå…­ç¨®äººæ ¼é¡å‹æ¸¬é©—ï¼Œæ‰¾å‡ºä½ çš„è·å ´èº«ä»½åŸå‹ã€‚å¥½æ™‚æœ‰å½±æä¾›å°ˆæ¥­èº«ä»½åŸå‹æ¸¬é©—ï¼Œå¹«åŠ©ä½ äº†è§£æœ€é©åˆçš„å½¢è±¡ç…§é¢¨æ ¼ï¼Œç«‹å³é–‹å§‹æ¸¬é©—ï¼Œç™¼ç¾ä½ çš„å°ˆæ¥­æ°£è³ªã€‚',
        'keywords': 'èº«ä»½åŸå‹æ¸¬é©—, RIASECæ¸¬é©—, è·å ´äººæ ¼æ¸¬é©—, å½¢è±¡ç…§é¢¨æ ¼, è·å ´æ°£è³ªæ¸¬è©¦',
    },
    'guide/makeup-and-hair.njk': {
        'title': 'å°ˆæ¥­å¦é«®æœå‹™ï½œå½¢è±¡ç…§å¦é«®é€ å‹ - å¥½æ™‚æœ‰å½±',
        'description': 'å¥½æ™‚æœ‰å½±æä¾›å°ˆæ¥­å¦é«®æœå‹™ï¼Œç”±ç¶“é©—è±å¯Œçš„å½©å¦å¸«æ‰“é€ è‡ªç„¶å¦æ„Ÿï¼Œè®“æ¯ä¸€ä½é¡§å®¢å±•ç¾è‡ªä¿¡é¢¨é‡‡ã€‚å¦é«®æœå‹™åŒ…å«åœ¨æ‹æ”è²»ç”¨ä¸­ï¼Œç«‹å³é ç´„é«”é©—å°ˆæ¥­å¦é«®é€ å‹æœå‹™ã€‚',
        'keywords': 'å½¢è±¡ç…§å¦é«®, è­‰ä»¶ç…§åŒ–å¦, å°ˆæ¥­å½©å¦, æ”å½±å¦é«®, å°åŒ—åŒ–å¦å¸«, å½¢è±¡ç…§é€ å‹',
    },
    'guide/faq.njk': {
        'title': 'å¸¸è¦‹å•é¡Œ FAQï½œé ç´„æ‹æ”ã€ä¿®åœ–å–ä»¶èªªæ˜ - å¥½æ™‚æœ‰å½±',
        'description': 'å¥½æ™‚æœ‰å½±å¸¸è¦‹å•é¡Œè§£ç­”ï¼šé ç´„æµç¨‹ã€æ‹æ”æ³¨æ„äº‹é …ã€ä¿®åœ–æœå‹™ã€å–ä»¶æ–¹å¼ç­‰ã€‚å®Œæ•´è§£ç­”æ‚¨çš„ç–‘å•ï¼Œè®“æ‚¨è¼•é¬†äº†è§£æœå‹™æµç¨‹ï¼Œç«‹å³æŸ¥çœ‹FAQï¼Œæˆ–ç›´æ¥é ç´„é«”é©—å°ˆæ¥­å½¢è±¡ç…§æœå‹™ã€‚',
        'keywords': 'å¥½æ™‚æœ‰å½±FAQ, å½¢è±¡ç…§å¸¸è¦‹å•é¡Œ, è­‰ä»¶ç…§é ç´„, æ‹æ”æµç¨‹, ä¿®åœ–æœå‹™, å–ä»¶æ–¹å¼',
    },
}


def parse_front_matter(content: str) -> Tuple[Dict, str, str]:
    """è§£æ front matterï¼Œè¿”å› (front_matter_dict, front_matter_text, body)"""
    front_matter_match = re.match(r'^(---\s*\n)(.*?)(\n---\s*\n)(.*)$', content, re.DOTALL)
    
    if not front_matter_match:
        return {}, '', content
    
    front_matter_start = front_matter_match.group(1)
    front_matter_text = front_matter_match.group(2)
    front_matter_end = front_matter_match.group(3)
    body = front_matter_match.group(4)
    
    # ç°¡å–®è§£æ YAML
    front_matter = {}
    current_key = None
    current_value = []
    in_nested = False
    nested_key = None
    nested_level = 0
    
    for line in front_matter_text.split('\n'):
        stripped = line.strip()
        if not stripped or stripped.startswith('#'):
            if current_value:
                current_value.append('')
            continue
        
        indent = len(line) - len(line.lstrip())
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯åµŒå¥—çµæ§‹ï¼ˆå¦‚ seo:ï¼‰
        if ':' in line and not line.strip().startswith('|'):
            colon_idx = line.index(':')
            key = line[:colon_idx].strip()
            value_part = line[colon_idx + 1:].strip()
            
            if not value_part or value_part in ('|', '>'):
                # é€™æ˜¯åµŒå¥—å°è±¡çš„é–‹å§‹
                in_nested = True
                nested_key = key
                nested_level = indent
                if key not in front_matter:
                    front_matter[key] = {}
                continue
            else:
                # æ™®é€šéµå€¼å°
                if in_nested and indent > nested_level:
                    # åµŒå¥—å°è±¡å…§çš„éµå€¼å°
                    if nested_key:
                        value = value_part.strip('"\'')
                        front_matter[nested_key][key] = value
                    continue
                else:
                    # é ‚å±¤éµå€¼å°
                    in_nested = False
                    nested_key = None
                    if current_key:
                        front_matter[current_key] = '\n'.join(current_value).strip().strip('"\'')
                    current_key = key
                    current_value = [value_part]
        else:
            # ç¹¼çºŒç•¶å‰çš„å€¼ï¼ˆå¯èƒ½æ˜¯å¤šè¡Œå­—ç¬¦ä¸²ï¼‰
            if current_key:
                current_value.append(line[indent:])
    
    if current_key:
        front_matter[current_key] = '\n'.join(current_value).strip().strip('"\'')
    
    return front_matter, front_matter_text, body


def update_seo_fields(front_matter_text: str, improvements: Dict) -> str:
    """æ›´æ–° front matter ä¸­çš„ SEO å­—æ®µ"""
    lines = front_matter_text.split('\n')
    result_lines = []
    i = 0
    title_updated = False
    seo_section_start = None
    seo_section_end = None
    in_seo_section = False
    
    while i < len(lines):
        line = lines[i]
        stripped = line.strip()
        
        # æª¢æŸ¥ title è¡Œ
        if stripped.startswith('title:') and 'title' in improvements:
            # æ›´æ–° title
            result_lines.append(f'title: "{improvements["title"]}"')
            title_updated = True
            i += 1
            continue
        
        # æª¢æŸ¥ seo: è¡Œ
        if stripped == 'seo:' or stripped.startswith('seo:'):
            seo_section_start = len(result_lines)
            in_seo_section = True
            result_lines.append('seo:')
            i += 1
            # è·³éåµŒå¥—å…§å®¹ï¼Œæˆ‘å€‘æœƒæ›¿æ›å®ƒ
            indent = len(line) - len(line.lstrip())
            while i < len(lines):
                next_line = lines[i]
                next_indent = len(next_line) - len(next_line.lstrip())
                if next_indent <= indent and next_line.strip():
                    break
                i += 1
            # æ·»åŠ æ–°çš„ seo å…§å®¹
            result_lines.append(f'  description: "{improvements.get("description", "")}"')
            result_lines.append(f'  keywords: "{improvements.get("keywords", "")}"')
            continue
        
        result_lines.append(line)
        i += 1
    
    # å¦‚æœ title æ²’æœ‰è¢«æ›´æ–°ï¼Œæ·»åŠ å®ƒ
    if not title_updated and 'title' in improvements:
        # åœ¨ layout ä¹‹å¾Œæ·»åŠ  title
        for j, line in enumerate(result_lines):
            if line.strip().startswith('layout:'):
                result_lines.insert(j + 1, f'title: "{improvements["title"]}"')
                break
    
    # å¦‚æœ seo éƒ¨åˆ†æ²’æœ‰æ‰¾åˆ°ï¼Œæ·»åŠ å®ƒ
    if seo_section_start is None and ('description' in improvements or 'keywords' in improvements):
        result_lines.append('seo:')
        result_lines.append(f'  description: "{improvements.get("description", "")}"')
        result_lines.append(f'  keywords: "{improvements.get("keywords", "")}"')
    
    return '\n'.join(result_lines)


def optimize_page(file_path: Path, improvements: Dict) -> bool:
    """å„ªåŒ–å–®å€‹é é¢"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è§£æ front matter
        front_matter_match = re.match(r'^(---\s*\n)(.*?)(\n---\s*\n)(.*)$', content, re.DOTALL)
        
        if not front_matter_match:
            print(f"  âš ï¸  ç„¡æ³•è§£æ front matter: {file_path.name}")
            return False
        
        front_matter_start = front_matter_match.group(1)
        front_matter_text = front_matter_match.group(2)
        front_matter_end = front_matter_match.group(3)
        body = front_matter_match.group(4)
        
        # æ›´æ–° SEO å­—æ®µ
        updated_front_matter = update_seo_fields(front_matter_text, improvements)
        
        # é‡çµ„å…§å®¹
        new_content = f"{front_matter_start}{updated_front_matter}{front_matter_end}{body}"
        
        # å¯«å›æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        return True
    except Exception as e:
        print(f"  âŒ éŒ¯èª¤: {e}")
        return False


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹ SEO å„ªåŒ–...\n")
    
    updated_count = 0
    skipped_count = 0
    
    for rel_path, improvements in SEO_IMPROVEMENTS.items():
        file_path = SRC_DIR / rel_path
        
        if not file_path.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {rel_path}")
            skipped_count += 1
            continue
        
        print(f"ğŸ“ å„ªåŒ–: {rel_path}")
        
        if optimize_page(file_path, improvements):
            print(f"  âœ… å®Œæˆ")
            updated_count += 1
        else:
            print(f"  âŒ å¤±æ•—")
            skipped_count += 1
        print()
    
    print("="*60)
    print(f"âœ… å„ªåŒ–å®Œæˆ: {updated_count} å€‹æ–‡ä»¶")
    if skipped_count > 0:
        print(f"âš ï¸  è·³é: {skipped_count} å€‹æ–‡ä»¶")
    print("="*60)
    print("\nğŸ’¡ å»ºè­°ï¼šé‹è¡Œ SEO å¯©è¨ˆè…³æœ¬é©—è­‰å„ªåŒ–æ•ˆæœ")
    print("   python3 scripts/seo-audit.py")


if __name__ == '__main__':
    main()
